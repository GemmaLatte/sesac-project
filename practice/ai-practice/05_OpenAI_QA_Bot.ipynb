{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp==3.8.5 (from -r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.8.5-cp38-cp38-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 2))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting appnope==0.1.3 (from -r requirements.txt (line 3))\n",
      "  Using cached appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting asttokens==2.2.1 (from -r requirements.txt (line 4))\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout==4.0.2 (from -r requirements.txt (line 5))\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs==22.2.0 (from -r requirements.txt (line 6))\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.11.1 (from -r requirements.txt (line 8))\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting blobfile==2.0.1 (from -r requirements.txt (line 9))\n",
      "  Using cached blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
      "Collecting bs4==0.0.1 (from -r requirements.txt (line 10))\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting certifi==2023.7.22 (from -r requirements.txt (line 11))\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==2.1.1 (from -r requirements.txt (line 12))\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting comm==0.1.2 (from -r requirements.txt (line 13))\n",
      "  Using cached comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
      "Collecting contourpy==1.0.7 (from -r requirements.txt (line 14))\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-win_amd64.whl (162 kB)\n",
      "     ---------------------------------------- 0.0/163.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/163.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 163.0/163.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting cycler==0.11.0 (from -r requirements.txt (line 15))\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting debugpy==1.6.5 (from -r requirements.txt (line 16))\n",
      "  Downloading debugpy-1.6.5-cp38-cp38-win_amd64.whl (4.9 MB)\n",
      "     ---------------------------------------- 0.0/4.9 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.9 MB 3.3 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.4/4.9 MB 4.6 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.8/4.9 MB 6.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.1/4.9 MB 5.6 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 1.4/4.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.9/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 2.2/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.4/4.9 MB 6.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.8/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 3.1/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 3.3/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.5/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.7/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.9/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 4.0/4.9 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.0/4.9 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 4.1/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 4.3/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.5/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 4.7/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.8/4.9 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.9/4.9 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Collecting docopt==0.6.2 (from -r requirements.txt (line 18))\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting entrypoints==0.4 (from -r requirements.txt (line 19))\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting executing==1.2.0 (from -r requirements.txt (line 20))\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting filelock==3.9.0 (from -r requirements.txt (line 21))\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting fonttools==4.38.0 (from -r requirements.txt (line 22))\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting frozenlist==1.3.3 (from -r requirements.txt (line 23))\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Collecting huggingface-hub>=0.0.12 (from -r requirements.txt (line 24))\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting idna==3.4 (from -r requirements.txt (line 25))\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting ipykernel==6.20.1 (from -r requirements.txt (line 26))\n",
      "  Using cached ipykernel-6.20.1-py3-none-any.whl (149 kB)\n",
      "Collecting ipython==8.10.0 (from -r requirements.txt (line 27))\n",
      "  Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
      "Collecting jedi==0.18.2 (from -r requirements.txt (line 28))\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting joblib==1.2.0 (from -r requirements.txt (line 29))\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting jupyter_client==7.4.8 (from -r requirements.txt (line 30))\n",
      "  Using cached jupyter_client-7.4.8-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_core==5.1.3 (from -r requirements.txt (line 31))\n",
      "  Using cached jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n",
      "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 32))\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.4/55.4 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting lxml==4.9.2 (from -r requirements.txt (line 33))\n",
      "  Downloading lxml-4.9.2-cp38-cp38-win_amd64.whl (3.9 MB)\n",
      "     ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.2/3.9 MB 6.9 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.3/3.9 MB 3.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.5/3.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.6/3.9 MB 1.5 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 0.6/3.9 MB 1.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.8/3.9 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 1.0/3.9 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 1.1/3.9 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.3/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.4/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 1.5/3.9 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.6/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.8/3.9 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 2.1/3.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 2.2/3.9 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.4/3.9 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.7/3.9 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.9/3.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.0/3.9 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.1/3.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.4/3.9 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.7/3.9 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.9/3.9 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib==3.6.3 (from -r requirements.txt (line 34))\n",
      "  Downloading matplotlib-3.6.3-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/7.2 MB 7.0 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.6/7.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.8/7.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.1/7.2 MB 5.8 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.4/7.2 MB 5.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.7/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 2.0/7.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.4/7.2 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 2.7/7.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.1/7.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 3.5/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.8/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.1/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.4/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 4.7/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.1/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 5.6/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.0/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.6/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 6.7/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.0/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.2/7.2 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 35)) (0.1.6)\n",
      "Collecting multidict==6.0.4 (from -r requirements.txt (line 36))\n",
      "  Downloading multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting nest-asyncio==1.5.6 (from -r requirements.txt (line 37))\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting numpy==1.24.1 (from -r requirements.txt (line 38))\n",
      "  Downloading numpy-1.24.1-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.6/14.9 MB 13.1 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 1.0/14.9 MB 10.9 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.3/14.9 MB 9.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.8/14.9 MB 9.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.3/14.9 MB 6.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.5/14.9 MB 6.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.8/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.3/14.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.6/14.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.8/14.9 MB 6.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.1/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.6/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.9/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.2/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.2/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.4/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.7/14.9 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.1/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.4/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.8/14.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.4/14.9 MB 5.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.8/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.0/14.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.4/14.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 8.7/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 9.2/14.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.5/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 10.0/14.9 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.3/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 10.8/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.2/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 11.7/14.9 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 12.1/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 12.6/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.0/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.3/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 13.7/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 14.1/14.9 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.7/14.9 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.9 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.9/14.9 MB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openai in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 39)) (1.10.0)\n",
      "Collecting packaging==23.0 (from -r requirements.txt (line 40))\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.7/42.7 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting pandas (from -r requirements.txt (line 41))\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: parso==0.8.3 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 42)) (0.8.3)\n",
      "Collecting pexpect==4.8.0 (from -r requirements.txt (line 43))\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.0/59.0 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 44)) (0.7.5)\n",
      "Collecting Pillow==9.4.0 (from -r requirements.txt (line 45))\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/2.5 MB 4.6 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.5 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.9/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.1/2.5 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.1/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.2/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.3/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.4/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.5/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.8/2.5 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.9/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.2/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.4/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting pipreqs==0.4.12 (from -r requirements.txt (line 46))\n",
      "  Downloading pipreqs-0.4.12-py2.py3-none-any.whl (32 kB)\n",
      "Collecting platformdirs==2.6.2 (from -r requirements.txt (line 47))\n",
      "  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting plotly==5.12.0 (from -r requirements.txt (line 48))\n",
      "  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ---------------------------------------- 0.0/15.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/15.2 MB 3.5 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/15.2 MB 4.5 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.6/15.2 MB 4.0 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.7/15.2 MB 3.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/15.2 MB 3.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.1/15.2 MB 4.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.5/15.2 MB 4.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.9/15.2 MB 5.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.1/15.2 MB 5.1 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.1/15.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.4/15.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.8/15.2 MB 5.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 3.2/15.2 MB 5.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.7/15.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 4.0/15.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.4/15.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.8/15.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 5.1/15.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 5.3/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.5/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.9/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.3/15.2 MB 6.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.6/15.2 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 7.0/15.2 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.4/15.2 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.7/15.2 MB 6.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.1/15.2 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.6/15.2 MB 6.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 9.0/15.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.2/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.6/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 10.1/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.5/15.2 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 10.9/15.2 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 11.3/15.2 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.6/15.2 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 12.1/15.2 MB 7.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 12.6/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.0/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.3/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.7/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 14.1/15.2 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 14.8/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.2/15.2 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting prompt-toolkit==3.0.36 (from -r requirements.txt (line 49))\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "     ---------------------------------------- 0.0/386.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 386.4/386.4 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting psutil==5.9.4 (from -r requirements.txt (line 50))\n",
      "  Downloading psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "     ---------------------------------------- 0.0/252.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 252.5/252.5 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting ptyprocess==0.7.0 (from -r requirements.txt (line 51))\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 52)) (0.2.2)\n",
      "Collecting pycryptodomex==3.17 (from -r requirements.txt (line 53))\n",
      "  Downloading pycryptodomex-3.17-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.5/1.7 MB 11.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.9/1.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.4/1.7 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting Pygments==2.15.0 (from -r requirements.txt (line 54))\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.4/1.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.8/1.1 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting pyparsing==3.0.9 (from -r requirements.txt (line 55))\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.3/98.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Collecting pytz==2022.7.1 (from -r requirements.txt (line 57))\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ---------------------------------------- 0.0/499.4 kB ? eta -:--:--\n",
      "     ------------------------------------  491.5/499.4 kB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 499.4/499.4 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting PyYAML==6.0 (from -r requirements.txt (line 58))\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     ---------------------------------------- 0.0/155.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 155.4/155.4 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting pyzmq==24.0.1 (from -r requirements.txt (line 59))\n",
      "  Downloading pyzmq-24.0.1-cp38-cp38-win_amd64.whl (998 kB)\n",
      "     ---------------------------------------- 0.0/999.0 kB ? eta -:--:--\n",
      "     ---------- --------------------------- 286.7/999.0 kB 5.9 MB/s eta 0:00:01\n",
      "     --------------- ---------------------- 399.4/999.0 kB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 788.5/999.0 kB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 952.3/999.0 kB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 999.0/999.0 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting regex==2022.10.31 (from -r requirements.txt (line 60))\n",
      "  Downloading regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/267.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 267.7/267.7 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting requests==2.31.0 (from -r requirements.txt (line 61))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scikit-learn==1.2.0 (from -r requirements.txt (line 62))\n",
      "  Downloading scikit_learn-1.2.0-cp38-cp38-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/8.2 MB 6.5 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.5/8.2 MB 5.7 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.8/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.1/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.3/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.6/8.2 MB 5.8 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.9/8.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.1/8.2 MB 5.8 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.1/8.2 MB 5.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.4/8.2 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 2.7/8.2 MB 5.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 3.1/8.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 3.4/8.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.8/8.2 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 4.2/8.2 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.6/8.2 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 5.0/8.2 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 5.3/8.2 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.6/8.2 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.1/8.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 6.4/8.2 MB 6.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.9/8.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 7.2/8.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.8/8.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.2/8.2 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting scipy==1.10.0 (from -r requirements.txt (line 63))\n",
      "  Downloading scipy-1.10.0-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.4/42.2 MB 8.9 MB/s eta 0:00:05\n",
      "      --------------------------------------- 0.8/42.2 MB 8.9 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.3/42.2 MB 9.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.7/42.2 MB 9.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 2.0/42.2 MB 8.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.3/42.2 MB 8.0 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.5/42.2 MB 7.5 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 2.7/42.2 MB 7.3 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 3.1/42.2 MB 7.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.2/42.2 MB 6.8 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.4/42.2 MB 6.5 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.5/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 3.8/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.1/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 4.3/42.2 MB 6.1 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 4.7/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 5.0/42.2 MB 6.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.3/42.2 MB 6.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.5/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.9/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.0/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 6.6/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.0/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.2/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.7/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.9/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.9/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.1/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.4/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 9.0/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 9.6/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.1/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.5/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 10.9/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 11.3/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 11.6/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.0/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.3/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.5/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 13.0/42.2 MB 6.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 13.5/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 13.6/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 13.9/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 14.2/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 14.6/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 14.8/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.1/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.5/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.8/42.2 MB 6.2 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.2/42.2 MB 6.2 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.5/42.2 MB 6.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.8/42.2 MB 6.3 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 17.0/42.2 MB 6.3 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 17.3/42.2 MB 6.2 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 17.7/42.2 MB 6.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 18.1/42.2 MB 6.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 18.6/42.2 MB 6.9 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 19.0/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 19.3/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 19.8/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 20.3/42.2 MB 7.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 20.6/42.2 MB 7.2 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 21.0/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 21.4/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 22.0/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.2/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.4/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.8/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 23.1/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.2/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.5/42.2 MB 7.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.7/42.2 MB 7.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 24.0/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 24.2/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.4/42.2 MB 7.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.5/42.2 MB 7.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.6/42.2 MB 6.7 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.7/42.2 MB 6.6 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.8/42.2 MB 6.5 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.9/42.2 MB 6.4 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 25.0/42.2 MB 6.3 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 25.1/42.2 MB 6.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.3/42.2 MB 6.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.5/42.2 MB 5.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.6/42.2 MB 5.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.8/42.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.9/42.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.0/42.2 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.3/42.2 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.3/42.2 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 26.4/42.2 MB 5.2 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 26.7/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 27.0/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 27.1/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 27.4/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 27.7/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 28.0/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 28.4/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 28.7/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.1/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.5/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 30.0/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 30.5/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 30.9/42.2 MB 4.6 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.0/42.2 MB 4.6 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.1/42.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.3/42.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.6/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 31.9/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.1/42.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.4/42.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.7/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.1/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.5/42.2 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.0/42.2 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.3/42.2 MB 4.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.7/42.2 MB 4.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 34.9/42.2 MB 4.9 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.1/42.2 MB 4.9 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.3/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.5/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.6/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.8/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.1/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.2/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.4/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.6/42.2 MB 5.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.9/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.1/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.3/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.3/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.5/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.6/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.0/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.4/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.8/42.2 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.2/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.4/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.7/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 40.0/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.3/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.6/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.8/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.0/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.2/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.6/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.2 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.2/42.2 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.2/42.2 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.2/42.2 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Collecting soupsieve==2.3.2.post1 (from -r requirements.txt (line 65))\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: stack-data==0.6.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 66)) (0.6.2)\n",
      "Collecting tenacity==8.1.0 (from -r requirements.txt (line 67))\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 68))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tiktoken==0.1.2 (from -r requirements.txt (line 69))\n",
      "  Downloading tiktoken-0.1.2-cp38-cp38-win_amd64.whl (575 kB)\n",
      "     ---------------------------------------- 0.0/575.5 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 61.4/575.5 kB 3.2 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 245.8/575.5 kB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 399.4/575.5 kB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 522.2/575.5 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 575.5/575.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tokenizers==0.13.2 (from -r requirements.txt (line 70))\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.3 MB 6.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.6/3.3 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.9/3.3 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.1/3.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.3/3.3 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.6/3.3 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.0/3.3 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.4/3.3 MB 3.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.8/3.3 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.0/3.3 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.3/3.3 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.3/3.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting tornado==6.3.3 (from -r requirements.txt (line 71))\n",
      "  Downloading tornado-6.3.3-cp38-abi3-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 72))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Collecting traitlets==5.8.1 (from -r requirements.txt (line 73))\n",
      "  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 116.8/116.8 kB ? eta 0:00:00\n",
      "Collecting transformers==4.30.0 (from -r requirements.txt (line 74))\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 6.5 MB/s eta 0:00:00\n",
      "Collecting typing_extensions==4.4.0 (from -r requirements.txt (line 75))\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting urllib3==1.26.13 (from -r requirements.txt (line 76))\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     ---------------------------------------- 0.0/140.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 140.6/140.6 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting wcwidth==0.2.5 (from -r requirements.txt (line 77))\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting yarg==0.1.9 (from -r requirements.txt (line 78))\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Collecting yarl==1.8.2 (from -r requirements.txt (line 79))\n",
      "  Downloading yarl-1.8.2-cp38-cp38-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.9/56.9 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from ipython==8.10.0->-r requirements.txt (line 27)) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from jupyter_core==5.1.3->-r requirements.txt (line 31)) (227)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.30.0->-r requirements.txt (line 74))\n",
      "  Downloading safetensors-0.4.2-cp38-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.0.12->-r requirements.txt (line 24))\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openai (from -r requirements.txt (line 39))\n",
      "  Using cached openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached openai-1.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: pip is still looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading openai-1.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.9-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.8-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai->-r requirements.txt (line 39))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting openai (from -r requirements.txt (line 39))\n",
      "  Downloading openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->-r requirements.txt (line 41))\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading aiohttp-3.8.5-cp38-cp38-win_amd64.whl (327 kB)\n",
      "   ---------------------------------------- 0.0/327.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 327.9/327.9 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 158.3/158.3 kB 9.9 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading tornado-6.3.3-cp38-abi3-win_amd64.whl (429 kB)\n",
      "   ---------------------------------------- 0.0/429.2 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 368.6/429.2 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 368.6/429.2 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 429.2/429.2 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/7.2 MB 7.3 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.5/7.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/7.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.4/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.5/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.9/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.1/7.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.4/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.6/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.1/7.2 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.6/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.9/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.4/7.2 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.2 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.3/7.2 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.2/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.4/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 10.3 MB/s eta 0:00:00\n",
      "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/77.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.0/77.0 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.8 MB 9.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/10.8 MB 10.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.2/10.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/10.8 MB 7.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/10.8 MB 7.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.2/10.8 MB 6.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/10.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.8 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.8 MB 7.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.1/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.4/10.8 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.9/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.0/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.9/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.7/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.4/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 7.6 MB/s eta 0:00:00\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Downloading safetensors-0.4.2-cp38-none-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.7/268.7 kB 8.3 MB/s eta 0:00:00\n",
      "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: bs4, docopt\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1265 sha256=35d5cb9b550d8dc23db1ca1fd8fb2fa99a2019c0edc7ae71ed3caa14994071d7\n",
      "  Stored in directory: c:\\users\\r2com\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=0ed17b18f55f0ae7b0d69ccf7e0043378dd970e6de1f37f94a718fd7f71a1aed\n",
      "  Stored in directory: c:\\users\\r2com\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built bs4 docopt\n",
      "Installing collected packages: wcwidth, tokenizers, pytz, ptyprocess, executing, docopt, appnope, urllib3, tzdata, typing_extensions, traitlets, tqdm, tornado, threadpoolctl, tenacity, soupsieve, safetensors, regex, pyzmq, PyYAML, pyparsing, Pygments, pycryptodomex, psutil, prompt-toolkit, platformdirs, Pillow, pexpect, packaging, numpy, nest-asyncio, multidict, lxml, kiwisolver, joblib, jedi, idna, fsspec, frozenlist, fonttools, filelock, entrypoints, debugpy, cycler, charset-normalizer, certifi, attrs, async-timeout, asttokens, yarl, scipy, requests, plotly, pandas, jupyter_core, contourpy, comm, blobfile, beautifulsoup4, aiosignal, yarg, tiktoken, scikit-learn, matplotlib, jupyter_client, ipython, huggingface-hub, bs4, aiohttp, transformers, pipreqs, openai, ipykernel\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.13\n",
      "    Uninstalling wcwidth-0.2.13:\n",
      "      Successfully uninstalled wcwidth-0.2.13\n",
      "  Attempting uninstall: executing\n",
      "    Found existing installation: executing 2.0.1\n",
      "    Uninstalling executing-2.0.1:\n",
      "      Successfully uninstalled executing-2.0.1\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.14.1\n",
      "    Uninstalling traitlets-5.14.1:\n",
      "      Successfully uninstalled traitlets-5.14.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.2\n",
      "    Uninstalling tornado-6.2:\n",
      "      Successfully uninstalled tornado-6.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 25.1.2\n",
      "    Uninstalling pyzmq-25.1.2:\n",
      "      Successfully uninstalled pyzmq-25.1.2\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.17.2\n",
      "    Uninstalling Pygments-2.17.2:\n",
      "      Successfully uninstalled Pygments-2.17.2\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.0\n",
      "    Uninstalling psutil-5.9.0:\n",
      "      Successfully uninstalled psutil-5.9.0\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.42\n",
      "    Uninstalling prompt-toolkit-3.0.42:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.42\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest_asyncio 1.6.0\n",
      "    Uninstalling nest_asyncio-1.6.0:\n",
      "      Successfully uninstalled nest_asyncio-1.6.0\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.1.0\n",
      "    Uninstalling lxml-5.1.0:\n",
      "      Successfully uninstalled lxml-5.1.0\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.3.2\n",
      "    Uninstalling joblib-1.3.2:\n",
      "      Successfully uninstalled joblib-1.3.2\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.1\n",
      "    Uninstalling jedi-0.19.1:\n",
      "      Successfully uninstalled jedi-0.19.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.47.2\n",
      "    Uninstalling fonttools-4.47.2:\n",
      "      Successfully uninstalled fonttools-4.47.2\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.6.7\n",
      "    Uninstalling debugpy-1.6.7:\n",
      "      Successfully uninstalled debugpy-1.6.7\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: asttokens\n",
      "    Found existing installation: asttokens 2.4.1\n",
      "    Uninstalling asttokens-2.4.1:\n",
      "      Successfully uninstalled asttokens-2.4.1\n",
      "  Attempting uninstall: jupyter_core\n",
      "    Found existing installation: jupyter_core 4.12.0\n",
      "    Uninstalling jupyter_core-4.12.0:\n",
      "      Successfully uninstalled jupyter_core-4.12.0\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.1.1\n",
      "    Uninstalling contourpy-1.1.1:\n",
      "      Successfully uninstalled contourpy-1.1.1\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.2.1\n",
      "    Uninstalling comm-0.2.1:\n",
      "      Successfully uninstalled comm-0.2.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.4\n",
      "    Uninstalling matplotlib-3.7.4:\n",
      "      Successfully uninstalled matplotlib-3.7.4\n",
      "  Attempting uninstall: jupyter_client\n",
      "    Found existing installation: jupyter_client 8.6.0\n",
      "    Uninstalling jupyter_client-8.6.0:\n",
      "      Successfully uninstalled jupyter_client-8.6.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.12.0\n",
      "    Uninstalling ipython-8.12.0:\n",
      "      Successfully uninstalled ipython-8.12.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.10.0\n",
      "    Uninstalling openai-1.10.0:\n",
      "      Successfully uninstalled openai-1.10.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.29.0\n",
      "    Uninstalling ipykernel-6.29.0:\n",
      "      Successfully uninstalled ipykernel-6.29.0\n",
      "Successfully installed Pillow-9.4.0 PyYAML-6.0 Pygments-2.15.0 aiohttp-3.8.5 aiosignal-1.3.1 appnope-0.1.3 asttokens-2.2.1 async-timeout-4.0.2 attrs-22.2.0 beautifulsoup4-4.11.1 blobfile-2.0.1 bs4-0.0.1 certifi-2023.7.22 charset-normalizer-2.1.1 comm-0.1.2 contourpy-1.0.7 cycler-0.11.0 debugpy-1.6.5 docopt-0.6.2 entrypoints-0.4 executing-1.2.0 filelock-3.9.0 fonttools-4.38.0 frozenlist-1.3.3 fsspec-2023.12.2 huggingface-hub-0.20.3 idna-3.4 ipykernel-6.20.1 ipython-8.10.0 jedi-0.18.2 joblib-1.2.0 jupyter_client-7.4.8 jupyter_core-5.1.3 kiwisolver-1.4.4 lxml-4.9.2 matplotlib-3.6.3 multidict-6.0.4 nest-asyncio-1.5.6 numpy-1.24.1 openai-0.28.1 packaging-23.0 pandas-2.0.3 pexpect-4.8.0 pipreqs-0.4.12 platformdirs-2.6.2 plotly-5.12.0 prompt-toolkit-3.0.36 psutil-5.9.4 ptyprocess-0.7.0 pycryptodomex-3.17 pyparsing-3.0.9 pytz-2022.7.1 pyzmq-24.0.1 regex-2022.10.31 requests-2.31.0 safetensors-0.4.2 scikit-learn-1.2.0 scipy-1.10.0 soupsieve-2.3.2.post1 tenacity-8.1.0 threadpoolctl-3.1.0 tiktoken-0.1.2 tokenizers-0.13.2 tornado-6.3.3 tqdm-4.64.1 traitlets-5.8.1 transformers-4.30.0 typing_extensions-4.4.0 tzdata-2023.4 urllib3-1.26.13 wcwidth-0.2.5 yarg-0.1.9 yarl-1.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~ornado'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~mq'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~sutil'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~ebugpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.5.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.4.0 which is incompatible.\n",
      "pydantic-core 2.14.6 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai 웹사이트 크롤링 하기. 우리는 각자 데이터 수집을 했으니 이 과정이 없어도 된다. 튜토리얼이므로 따라하는 것에 의미를 두자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding='utf-8') as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the text but remove the tags\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # Otherwise, write the text to the file in the text directory\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "https://openai.com/research\n",
      "https://openai.com/research/confidence-building-measures-for-artificial-intelligence\n",
      "https://openai.com/research/confidence-building-measures-for-artificial-intelligence#content\n",
      "https://openai.com/research?contentTypes=publication\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'text/openai.com/openai.com_research?contentTypes=publication.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 실제로는 시간 최소 20분 이상 걸리니 마음의 준비를 단단히 할 것.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 우리는 실습 목적이므로 3분 정도만 돌려보고 크롤링을 중단한다.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(url) \u001b[38;5;66;03m# for debugging and to see the progress\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Save text from the url to a <url>.txt file\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlocal_domain\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Get the text from the URL using BeautifulSoup\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Get the text but remove the tags\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Documents\\GitHub\\sesac-project\\.env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'text/openai.com/openai.com_research?contentTypes=publication.txt'"
     ]
    }
   ],
   "source": [
    "# 실제로는 시간 최소 20분 이상 걸리니 마음의 준비를 단단히 할 것.\n",
    "# 우리는 실습 목적이므로 3분 정도만 돌려보고 크롤링을 중단한다.\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2com\\AppData\\Local\\Temp\\ipykernel_16612\\2091664849.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xc2 in position 2001: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Open the file and read the text\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 11\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend((file[\u001b[38;5;241m11\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#update\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), text))\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xc2 in position 2001: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 텍스트 파일들 리스트로 만들기\n",
    "texts=[]\n",
    "\n",
    "# 모든 텍스트 파일 가져와서 내용 리스트에 붙이기\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "# 파일 경로와 텍스트로 데이터 프레임 만들기\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 개행문자 삭제해서 파일 이름과 함께 붙여서 text 칼럼에 넣기.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mfname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m remove_newlines(df\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/scraped.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# 개행문자 삭제해서 파일 이름과 함께 붙여서 text 칼럼에 넣기.\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래는 텍스트 임베딩을 가져오는데 소요되는 전체 토큰을 계산하는 코드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "#  cl100k_base는 토크나이저다. 임베딩과는 큰 관련이 없다.\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# 소요되는 토큰을 각 칼럼에 저장하기. apply 함수가 쓰였다!\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1239.000000\n",
       "mean     2116.348668\n",
       "std      1438.308043\n",
       "min         3.000000\n",
       "25%      1025.000000\n",
       "50%      1353.000000\n",
       "75%      3886.000000\n",
       "max      8258.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 히스토그램으로 그려서 보기\n",
    "df.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트가 최대 입력 토큰을 초과할 수도 있다. 그러므로, 최대 입력 길이만큼 쪼개주는 함수를 만들어야 한다.\n",
    "## text-ada-002의 경우, 최대 입력 길이가 4095이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2000\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # 일단 문장으로 쪼개기\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # 문장마다 몇 토큰 들어가는지 세어 주기.\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # 문장의 토큰 수와 현재까지의 토큰 수의 합이 최대 토큰 수를 초과하면, 해당 청크를 청크 목록에 추가하고 청크 및 현재까지의 토큰 수를 초기화한다.\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened = []\n",
    "# 데이터 프레임 순회하기\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'research?authors=henk tillman.  Research index    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Filter selectionsTopicsAdversarial examplesÂ\\xa0 (4)Audio generationÂ\\xa0 (2)CommunityÂ\\xa0 (12)ComputeÂ\\xa0 (8)Computer visionÂ\\xa0 (9)Contrastive learningÂ\\xa0 (2)Domain randomizationÂ\\xa0 (6)Dota 2Â\\xa0 (7)EnvironmentsÂ\\xa0 (10)ExplorationÂ\\xa0 (4)GamesÂ\\xa0 (19)Generative modelsÂ\\xa0 (19)Human feedbackÂ\\xa0 (11)Image generationÂ\\xa0 (5)InterpretabilityÂ\\xa0 (4)LanguageÂ\\xa0 (30)MemoryÂ\\xa0 (1)Meta-learningÂ\\xa0 (9)Multi-agentÂ\\xa0 (7)Open sourceÂ\\xa0 (20)Policy optimizationÂ\\xa0 (5)Procedural generationÂ\\xa0 (3)ReasoningÂ\\xa0 (3)Reinforcement learningÂ\\xa0 (56)Representation learningÂ\\xa0 (8)ResearchÂ\\xa0 (2)Responsible AIÂ\\xa0 (18)RoboticsÂ\\xa0 (18)RobustnessÂ\\xa0 (8)Safety & AlignmentÂ\\xa0 (35)Scaling propertiesÂ\\xa0 (3)Self-playÂ\\xa0 (9)Sim-to-realÂ\\xa0 (5)Software engineeringÂ\\xa0 (21)SparsityÂ\\xa0 (2)Speech recognitionÂ\\xa0 (1)SummarizationÂ\\xa0 (4)Supervised learningÂ\\xa0 (2)Transfer learningÂ\\xa0 (7)TransformersÂ\\xa0 (9)Unsupervised learningÂ\\xa0 (6)ModelsCLIPÂ\\xa0 (4)DactylÂ\\xa0 (2)DALLÂ·EÂ\\xa0 (1)DALLÂ·E 2Â\\xa0 (2)DALLÂ·E 3Â\\xa0 (1)GlowÂ\\xa0 (1)GPTÂ\\xa0 (1)GPT-2Â\\xa0 (5)GPT-3Â\\xa0 (6)GPT-4Â\\xa0 (3)JukeboxÂ\\xa0 (1)MuseNetÂ\\xa0 (1)OpenAI CodexÂ\\xa0 (3)OpenAI FiveÂ\\xa0 (7)Point-EÂ\\xa0 (1)WhisperÂ\\xa0 (1)TypesConclusionÂ\\xa0 (14)MilestoneÂ\\xa0 (33)PublicationÂ\\xa0 (132)ReleaseÂ\\xa0 (52)AuthorsMartÃ\\xadn AbadiÂ\\xa0 (1)Pieter AbbeelÂ\\xa0 (35)Joshua AchiamÂ\\xa0 (8)Steven AdlerÂ\\xa0 (2)Sandhini AgarwalÂ\\xa0 (5)Lama AhmadÂ\\xa0 (1)Ilge AkkayaÂ\\xa0 (2)Maruan Al-ShedivatÂ\\xa0 (4)Dario AmodeiÂ\\xa0 (16)Daniella AmodeiÂ\\xa0 (1)Daniela AmodeiÂ\\xa0 (1)Marcin AndrychowiczÂ\\xa0 (12)Leopold AschenbrennerÂ\\xa0 (1)Tasmin AsfourÂ\\xa0 (1)Amanda AskellÂ\\xa0 (4)Anish AthalyeÂ\\xa0 (1)Igor BabuschkinÂ\\xa0 (1)Bowen BakerÂ\\xa0 (6)Suchir BalajiÂ\\xa0 (2)Trapit BansalÂ\\xa0 (2)Yamini BansalÂ\\xa0 (1)Boaz BarakÂ\\xa0 (1)Elizabeth BarnesÂ\\xa0 (1)Ben BarryÂ\\xa0 (1)Peter L. BartlettÂ\\xa0 (1)Mohammad BavarianÂ\\xa0 (2)Alexandre M BayenÂ\\xa0 (1)Christopher BernerÂ\\xa0 (4)Jesse BettencourtÂ\\xa0 (1)Alex BeutelÂ\\xa0 (1)Lukas BiewaldÂ\\xa0 (1)Steven BillsÂ\\xa0 (1)Xue Bin PengÂ\\xa0 (2)Trevor BlackwellÂ\\xa0 (1)Greg BrockmanÂ\\xa0 (14)Tom BrownÂ\\xa0 (6)Miles BrundageÂ\\xa0 (8)Yura BurdaÂ\\xa0 (7)Collin BurnsÂ\\xa0 (1)Nick CammarataÂ\\xa0 (2)Rosie CampbellÂ\\xa0 (1)Andrew N. CarrÂ\\xa0 (1)Shan CarterÂ\\xa0 (2)Brooke ChanÂ\\xa0 (3)Fotios ChantzisÂ\\xa0 (1)Peter ChenÂ\\xa0 (3)Richard ChenÂ\\xa0 (6)Xi ChenÂ\\xa0 (7)Mark ChenÂ\\xa0 (7)Ricky T. Q. ChenÂ\\xa0 (1)Benjamin ChessÂ\\xa0 (3)Vicki CheungÂ\\xa0 (3)Rewon ChildÂ\\xa0 (4)Maciek ChociejÂ\\xa0 (4)Paul ChristianoÂ\\xa0 (9)Casey ChuÂ\\xa0 (1)Jack ClarkÂ\\xa0 (15)Jeff CluneÂ\\xa0 (1)Karl CobbeÂ\\xa0 (4)Taco CohenÂ\\xa0 (1)Dave CummingsÂ\\xa0 (1)Andrew M. DaiÂ\\xa0 (1)Trevor DarrellÂ\\xa0 (1)PrzemysÅ\\x82aw DÄ\\x99biakÂ\\xa0 (2)Akshay DegwekarÂ\\xa0 (1)Christy DennisonÂ\\xa0 (3)Filip De TurckÂ\\xa0 (1)Prafulla DhariwalÂ\\xa0 (9)Yilun DuÂ\\xa0 (2)Yan DuanÂ\\xa0 (12)David DuvenaudÂ\\xa0 (1)Harri EdwardsÂ\\xa0 (6)Alexei A. EfrosÂ\\xa0 (1)Tyna EloundouÂ\\xa0 (3)Ã\\x9alfar ErlingssonÂ\\xa0 (1)Owain EvansÂ\\xa0 (2)David FarhiÂ\\xa0 (2)Chelsea FinnÂ\\xa0 (1)Quirin FischerÂ\\xa0 (2)Carlos FlorensaÂ\\xa0 (1)Jakob FoersterÂ\\xa0 (3)Rachel FongÂ\\xa0 (3)Davis FooteÂ\\xa0 (1)Kevin FransÂ\\xa0 (1)Deep GanguliÂ\\xa0 (1)Leo GaoÂ\\xa0 (4)Jon GauthierÂ\\xa0 (1)Gabriel GohÂ\\xa0 (3)Ian GoodfellowÂ\\xa0 (5)Jonathan GordonÂ\\xa0 (1)Will GrathwohlÂ\\xa0 (1)Scott GrayÂ\\xa0 (8)Roger GrosseÂ\\xa0 (1)Aditya GroverÂ\\xa0 (1)Jayesh K.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv('../../.env'))\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "client = OpenAI()\n",
    "openai.api_key ='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt = df.text[0]\n",
    "res = client.embeddings.create(input=tmp_txt, model='text-embedding-ada-002')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       research?authors=henk tillman.  Research index...\n",
       "1       GuptaÂ  (1)William GussÂ  (1)Chris HallacyÂ  (...\n",
       "2       StanleyÂ  (1)Jacob SteinhardtÂ  (1)Nisan Stien...\n",
       "3       research?authors=arthur petron.  Research inde...\n",
       "4       GuptaÂ  (1)William GussÂ  (1)Chris HallacyÂ  (...\n",
       "                              ...                        \n",
       "2020    StanleyÂ  (1)Jacob SteinhardtÂ  (1)Nisan Stien...\n",
       "2021    research?authors=chelsea voss.  Research index...\n",
       "2022    GuptaÂ  (1)William GussÂ  (1)Chris HallacyÂ  (...\n",
       "2023    StanleyÂ  (1)Jacob SteinhardtÂ  (1)Nisan Stien...\n",
       "2024    jobs.  Careers    CloseSearch Submit Skip to m...\n",
       "Name: text, Length: 2025, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df.text.apply(lambda x: client.embeddings.create(input=x, model='text-embedding-ada-002').data[0].embedding)\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=henk tillman.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.006426393985748291, -0.020367804914712906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.0030676156748086214, -0.003532822011038661,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.00471889041364193, -0.0065433862619102, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0                     research?authors=henk tillman.         9   \n",
       "1  Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2  Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...       195   \n",
       "3  Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "4                    research?authors=arthur petron.         9   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.006426393985748291, -0.020367804914712906,...  \n",
       "1  [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2  [0.0030676156748086214, -0.003532822011038661,...  \n",
       "3  [0.0025212429463863373, -0.0010925385868176818...  \n",
       "4  [-0.00471889041364193, -0.0065433862619102, -0...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=henk tillman.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.006426393985748291, -0.020367804914712906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.0030676156748086214, -0.003532822011038661,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.00471889041364193, -0.0065433862619102, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>AI is an extremely powerful tool that must be ...</td>\n",
       "      <td>263</td>\n",
       "      <td>[-0.003355710068717599, -0.03532751277089119, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>research?topics=meta learning.</td>\n",
       "      <td>7</td>\n",
       "      <td>[-0.01474149152636528, -0.004488331265747547, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.00284423865377903, -0.003469044342637062, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  n_tokens  \\\n",
       "0                        research?authors=henk tillman.         9   \n",
       "1     Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2     Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...       195   \n",
       "3     Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "4                       research?authors=arthur petron.         9   \n",
       "...                                                 ...       ...   \n",
       "2020  AI is an extremely powerful tool that must be ...       263   \n",
       "2021                     research?topics=meta learning.         7   \n",
       "2022  Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2023  Efros (1)Tyna Eloundou (3)Ãlfar Erlingsson (1...       195   \n",
       "2024  Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [-0.006426393985748291, -0.020367804914712906,...  \n",
       "1     [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2     [0.0030676156748086214, -0.003532822011038661,...  \n",
       "3     [0.0025212429463863373, -0.0010925385868176818...  \n",
       "4     [-0.00471889041364193, -0.0065433862619102, -0...  \n",
       "...                                                 ...  \n",
       "2020  [-0.003355710068717599, -0.03532751277089119, ...  \n",
       "2021  [-0.01474149152636528, -0.004488331265747547, ...  \n",
       "2022  [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2023  [0.00284423865377903, -0.003469044342637062, 0...  \n",
       "2024  [0.0025212429463863373, -0.0010925385868176818...  \n",
       "\n",
       "[2025 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 프레임으로부터 가장 유사한 문맥을 찾아서 질문에 대한 문맥을 만든다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 질문으로부터 임베딩을 만든다.\n",
    "    q_embeddings = client.embeddings.create(input=question, model='text-embedding-ada-002').data[0].embedding\n",
    "    \n",
    "    # q_embeddings: \"최신 임베딩 모델이 뭐야?\" => 숫자로 들어가 있음.\n",
    "    \n",
    "    # df['embeddings']: 모든 문서들의 묶음. => 숫자로 들어가 있음.\n",
    "\n",
    "    # 임베딩으로부터 거리를 계산한다.\n",
    "    # cosine similarity: 문자열끼리 유사도 비교하는 것.\n",
    "    df[\"distances\"] = df[\"embeddings\"].apply(lambda x: cosine(q_embeddings, x))\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "    # 거리순으로 정렬하고, 문맥이 너무 길어지기 전까지만 텍스트를 문맥에 추가한다.\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "\n",
    "        # 현재 토큰 수에 텍스트 길이를 더한다.\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        # 문맥이 너무 길어지면, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "\n",
    "        # 아니면, 문맥에 추가한다.\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)\n",
    "\n",
    "\n",
    "# 이 함수가 하는 일 다시 정리해보자.\n",
    "# 1. 질문이 들어오면 질문을 임베딩한다. 즉, 숫자 표현으로 바꿔준다.\n",
    "# 2. 모든 문장들의 임베딩과 질문의 임베딩 사이의 거리를 계산한다.\n",
    "# 3. 문맥이 너무 길어지면 안되니까, 가장 거리가 가까운 문장부터 차례대로 문맥에 추가한다.\n",
    "# 4. 문맥을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research?authors=jong wook kim.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=diane yoon.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=joanne jang.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=heewoo jun.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=cathy yeh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=reiichiro nakano.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=teddy lee.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=mark chen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yonadav shavit.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yura burda.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yan duan.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=rachel lim.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=lama ahmad.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=peter chen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=nicolas papernot.\n",
      "\n",
      "###\n",
      "\n",
      "Kaelbling, Hyeonwoo Noh, Lerrel Pinto, John Schulman, Ilya Sutskever & Tao Xu.Video: Peter Jordan (Director), Yvette Solis (Producer), Brooke Chan (Producer)Editor: Ashley PilipiszynDesign: Justin Jay Wang & Ben BarryPhotography: Eric HainesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top .\n",
      "\n",
      "###\n",
      "\n",
      "Kaelbling, Hyeonwoo Noh, Lerrel Pinto, John Schulman, Ilya Sutskever & Tao Xu.Video: Peter Jordan (Director), Yvette Solis (Producer), Brooke Chan (Producer)Editor: Ashley PilipiszynDesign: Justin Jay Wang & Ben BarryPhotography: Eric HainesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top .\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=heidy khlaaf.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=susan zhang.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lama ahmad.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=rewon child.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=irene solaiman.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=brooke chan.\n",
      "\n",
      "###\n",
      "\n",
      "blog.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=kevin frans.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=ariel herbert voss.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=rachel fong.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=benjamin mann.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=lilian weng.\n",
      "\n",
      "###\n",
      "\n",
      "careers search?c=security.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=nisan stiennon.\n",
      "\n",
      "###\n",
      "\n",
      "enterprise.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=leo gao.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=gabriel goh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=catherine yeh.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=jade leung.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lilian weng.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=arvind neelakantan.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=vicki cheung.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=teddy lee.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jan hendrik kirchner.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=sandhini agarwal.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jonas schneider.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jonathan ho.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lerrel pinto.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jan leike.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=tyna eloundou.\n",
      "\n",
      "###\n",
      "\n",
      "The board of AlmannarÃ³mur.English to IcelandicPromptHvaÃ° heitir Donald Duck Ã¡ Ã­slensku?What is Donald Duck called in Icelandic?GPT-3 completionDonald Duck heitir Ã¡ Ã­slensku Donaldi KjÃ¡niDonald Duck is called in Icelandic Donaldi the FoolIncorrectChatGPT completionDonald Duck heitir sama nafni Ã¡ Ã­slensku og enskuDonald Duck has the same name in Icelandic and EnglishIncorrectGPT-4 completionDonald Duck heitir AndrÃ©s Ãnd Ã¡ Ã­slenskuDonald Duck is called AndrÃ©s Ãnd in IcelandicCorrectEven GPT-4, while much better at Icelandic than GPT-3.5, produces Icelandic with grammatical errors, âtranslationeseâ, and incorrect cultural knowledge. To change this, VilhjÃ¡lmur Ãorsteinsson, chief executive at MiÃ°eind ehf, a language technology company involved in the initiative, assembled a team of 40 volunteers to train GPT-4 on proper Icelandic grammar and cultural knowledge.In a process called Reinforcement Learning from Human Feedback, or RLHF, human testers give GPT-4 a prompt, and four possible completions are generated. Testers then select the best answer from the four responses and edit it to create an ideal completion. The data from this process is then used to further train GPT-4 to produce better responses in the future.Prior to RLHF, the process of fine-tuning a model was labor and data-intensive.\n",
      "\n",
      "###\n",
      "\n",
      "The board of AlmannarÃ³mur.English to IcelandicPromptHvaÃ° heitir Donald Duck Ã¡ Ã­slensku?What is Donald Duck called in Icelandic?GPT-3 completionDonald Duck heitir Ã¡ Ã­slensku Donaldi KjÃ¡niDonald Duck is called in Icelandic Donaldi the FoolIncorrectChatGPT completionDonald Duck heitir sama nafni Ã¡ Ã­slensku og enskuDonald Duck has the same name in Icelandic and EnglishIncorrectGPT-4 completionDonald Duck heitir AndrÃ©s Ãnd Ã¡ Ã­slenskuDonald Duck is called AndrÃ©s Ãnd in IcelandicCorrectEven GPT-4, while much better at Icelandic than GPT-3.5, produces Icelandic with grammatical errors, âtranslationeseâ, and incorrect cultural knowledge. To change this, VilhjÃ¡lmur Ãorsteinsson, chief executive at MiÃ°eind ehf, a language technology company involved in the initiative, assembled a team of 40 volunteers to train GPT-4 on proper Icelandic grammar and cultural knowledge.In a process called Reinforcement Learning from Human Feedback, or RLHF, human testers give GPT-4 a prompt, and four possible completions are generated. Testers then select the best answer from the four responses and edit it to create an ideal completion. The data from this process is then used to further train GPT-4 to produce better responses in the future.Prior to RLHF, the process of fine-tuning a model was labor and data-intensive.\n",
      "\n",
      "###\n",
      "\n",
      "scholars.    \n",
      "\n",
      "###\n",
      "\n",
      "careers search.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=john schulman 2.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=erika reinhardt.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=long ouyang.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=ilya sutskever.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=pranav shyam.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=alex paino.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=jan leike.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=harold nguyen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=oleg klimov.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=sandhini agarwal.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=melanie subbiah.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=vik goel.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=casey chu.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=ilya sutskever.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=aditya ramesh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=clemens winter.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=mateusz litwin.\n"
     ]
    }
   ],
   "source": [
    "print(create_context('너 이름 뭐야?', df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 프레임 텍스트에서 가장 유사한 문맥을 기반으로 질문에 답한다.\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n---\\n\\n Question: {question}\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What day is it?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The newest embeddings model is called \"text-embedding-ada-002\".'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is newest embeddings model?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, the pricing for using GPT-4 is as follows:\\n\\n- GPT-4 Turbo 8K Input: $0.03 Output: $0.06\\n- GPT-4 Turbo 32K Input: $0.06 Output: $0.12\\n- GPT-4 Turbo 128K Input: $0.01 Output: $0.03\\n\\nPlease note that these prices are per 1,000 tokens.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"How much does it cost to use gpt-4?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"최신 임베딩 모델이 뭐야?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'최신 임베딩 모델은 \"text-embedding-ada-002\"입니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is newest embeddings model? 한국말로 응답해줘\", debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05f34a34d73b71652304030c1097be3a5720ea2447153dd6542d145a26b73181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
