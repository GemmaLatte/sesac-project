{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp==3.8.5 (from -r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.8.5-cp38-cp38-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 2))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting appnope==0.1.3 (from -r requirements.txt (line 3))\n",
      "  Using cached appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting asttokens==2.2.1 (from -r requirements.txt (line 4))\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout==4.0.2 (from -r requirements.txt (line 5))\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs==22.2.0 (from -r requirements.txt (line 6))\n",
      "  Using cached attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.11.1 (from -r requirements.txt (line 8))\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting blobfile==2.0.1 (from -r requirements.txt (line 9))\n",
      "  Using cached blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
      "Collecting bs4==0.0.1 (from -r requirements.txt (line 10))\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting certifi==2023.7.22 (from -r requirements.txt (line 11))\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==2.1.1 (from -r requirements.txt (line 12))\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting comm==0.1.2 (from -r requirements.txt (line 13))\n",
      "  Using cached comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
      "Collecting contourpy==1.0.7 (from -r requirements.txt (line 14))\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-win_amd64.whl (162 kB)\n",
      "     ---------------------------------------- 0.0/163.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/163.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 163.0/163.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting cycler==0.11.0 (from -r requirements.txt (line 15))\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting debugpy==1.6.5 (from -r requirements.txt (line 16))\n",
      "  Downloading debugpy-1.6.5-cp38-cp38-win_amd64.whl (4.9 MB)\n",
      "     ---------------------------------------- 0.0/4.9 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.9 MB 3.3 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.4/4.9 MB 4.6 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.8/4.9 MB 6.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.1/4.9 MB 5.6 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 1.4/4.9 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.9/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 2.2/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.4/4.9 MB 6.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.8/4.9 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 3.1/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.2/4.9 MB 6.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 3.3/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.5/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.7/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.9/4.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 4.0/4.9 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.0/4.9 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 4.1/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 4.3/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.5/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 4.7/4.9 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.8/4.9 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.9/4.9 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Collecting docopt==0.6.2 (from -r requirements.txt (line 18))\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting entrypoints==0.4 (from -r requirements.txt (line 19))\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting executing==1.2.0 (from -r requirements.txt (line 20))\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting filelock==3.9.0 (from -r requirements.txt (line 21))\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting fonttools==4.38.0 (from -r requirements.txt (line 22))\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting frozenlist==1.3.3 (from -r requirements.txt (line 23))\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Collecting huggingface-hub>=0.0.12 (from -r requirements.txt (line 24))\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting idna==3.4 (from -r requirements.txt (line 25))\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting ipykernel==6.20.1 (from -r requirements.txt (line 26))\n",
      "  Using cached ipykernel-6.20.1-py3-none-any.whl (149 kB)\n",
      "Collecting ipython==8.10.0 (from -r requirements.txt (line 27))\n",
      "  Using cached ipython-8.10.0-py3-none-any.whl (784 kB)\n",
      "Collecting jedi==0.18.2 (from -r requirements.txt (line 28))\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting joblib==1.2.0 (from -r requirements.txt (line 29))\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting jupyter_client==7.4.8 (from -r requirements.txt (line 30))\n",
      "  Using cached jupyter_client-7.4.8-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_core==5.1.3 (from -r requirements.txt (line 31))\n",
      "  Using cached jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n",
      "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 32))\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.4/55.4 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting lxml==4.9.2 (from -r requirements.txt (line 33))\n",
      "  Downloading lxml-4.9.2-cp38-cp38-win_amd64.whl (3.9 MB)\n",
      "     ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.2/3.9 MB 6.9 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.3/3.9 MB 3.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.5/3.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.5/3.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.6/3.9 MB 1.5 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 0.6/3.9 MB 1.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.8/3.9 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 1.0/3.9 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 1.1/3.9 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.3/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.4/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 1.5/3.9 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.6/3.9 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.8/3.9 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 2.1/3.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 2.2/3.9 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.4/3.9 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.7/3.9 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.9/3.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.0/3.9 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 3.1/3.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.4/3.9 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.7/3.9 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.9/3.9 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib==3.6.3 (from -r requirements.txt (line 34))\n",
      "  Downloading matplotlib-3.6.3-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/7.2 MB 7.0 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 0.6/7.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.8/7.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.1/7.2 MB 5.8 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.4/7.2 MB 5.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.7/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 2.0/7.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.4/7.2 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 2.7/7.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.1/7.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 3.5/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.8/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.1/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.4/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 4.7/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.1/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 5.6/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.0/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.6/7.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 6.7/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.0/7.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.2/7.2 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 35)) (0.1.6)\n",
      "Collecting multidict==6.0.4 (from -r requirements.txt (line 36))\n",
      "  Downloading multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting nest-asyncio==1.5.6 (from -r requirements.txt (line 37))\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting numpy==1.24.1 (from -r requirements.txt (line 38))\n",
      "  Downloading numpy-1.24.1-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.6/14.9 MB 13.1 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 1.0/14.9 MB 10.9 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.3/14.9 MB 9.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.8/14.9 MB 9.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.9 MB 9.5 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.3/14.9 MB 6.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.5/14.9 MB 6.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.8/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.3/14.9 MB 6.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.6/14.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.8/14.9 MB 6.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.1/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.6/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.9/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.2/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.2/14.9 MB 6.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.4/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.7/14.9 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.1/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.4/14.9 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.8/14.9 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.3/14.9 MB 6.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.4/14.9 MB 5.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.8/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.0/14.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.4/14.9 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 8.7/14.9 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 9.2/14.9 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.5/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 10.0/14.9 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.3/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 10.8/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.2/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 11.7/14.9 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 12.1/14.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 12.6/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.0/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.3/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 13.7/14.9 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 14.1/14.9 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.7/14.9 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.8/14.9 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.9/14.9 MB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: openai in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 39)) (1.10.0)\n",
      "Collecting packaging==23.0 (from -r requirements.txt (line 40))\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.7/42.7 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting pandas (from -r requirements.txt (line 41))\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: parso==0.8.3 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 42)) (0.8.3)\n",
      "Collecting pexpect==4.8.0 (from -r requirements.txt (line 43))\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.0/59.0 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 44)) (0.7.5)\n",
      "Collecting Pillow==9.4.0 (from -r requirements.txt (line 45))\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/2.5 MB 4.6 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.8/2.5 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.9/2.5 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.1/2.5 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.1/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.2/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.3/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.4/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.5/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.8/2.5 MB 2.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.9/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.2/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.4/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting pipreqs==0.4.12 (from -r requirements.txt (line 46))\n",
      "  Downloading pipreqs-0.4.12-py2.py3-none-any.whl (32 kB)\n",
      "Collecting platformdirs==2.6.2 (from -r requirements.txt (line 47))\n",
      "  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting plotly==5.12.0 (from -r requirements.txt (line 48))\n",
      "  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ---------------------------------------- 0.0/15.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/15.2 MB 3.5 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.4/15.2 MB 4.5 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.6/15.2 MB 4.0 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.7/15.2 MB 3.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/15.2 MB 3.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.1/15.2 MB 4.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.5/15.2 MB 4.5 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.9/15.2 MB 5.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.1/15.2 MB 5.1 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 2.1/15.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.4/15.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.8/15.2 MB 5.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 3.2/15.2 MB 5.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.7/15.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 4.0/15.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.4/15.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.8/15.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 5.1/15.2 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 5.3/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.5/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.9/15.2 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 6.3/15.2 MB 6.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.6/15.2 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 7.0/15.2 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.4/15.2 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.7/15.2 MB 6.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.1/15.2 MB 6.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.6/15.2 MB 6.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 9.0/15.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.2/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.6/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 10.1/15.2 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 10.5/15.2 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 10.9/15.2 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 11.3/15.2 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 11.6/15.2 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 12.1/15.2 MB 7.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 12.6/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.0/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 13.3/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.7/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 14.1/15.2 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 14.8/15.2 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.2 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.2/15.2 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting prompt-toolkit==3.0.36 (from -r requirements.txt (line 49))\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "     ---------------------------------------- 0.0/386.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 386.4/386.4 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting psutil==5.9.4 (from -r requirements.txt (line 50))\n",
      "  Downloading psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "     ---------------------------------------- 0.0/252.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 252.5/252.5 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting ptyprocess==0.7.0 (from -r requirements.txt (line 51))\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 52)) (0.2.2)\n",
      "Collecting pycryptodomex==3.17 (from -r requirements.txt (line 53))\n",
      "  Downloading pycryptodomex-3.17-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.5/1.7 MB 11.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.9/1.7 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.4/1.7 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting Pygments==2.15.0 (from -r requirements.txt (line 54))\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.4/1.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.8/1.1 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting pyparsing==3.0.9 (from -r requirements.txt (line 55))\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 0.0/98.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.3/98.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Collecting pytz==2022.7.1 (from -r requirements.txt (line 57))\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ---------------------------------------- 0.0/499.4 kB ? eta -:--:--\n",
      "     ------------------------------------  491.5/499.4 kB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 499.4/499.4 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting PyYAML==6.0 (from -r requirements.txt (line 58))\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     ---------------------------------------- 0.0/155.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 155.4/155.4 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting pyzmq==24.0.1 (from -r requirements.txt (line 59))\n",
      "  Downloading pyzmq-24.0.1-cp38-cp38-win_amd64.whl (998 kB)\n",
      "     ---------------------------------------- 0.0/999.0 kB ? eta -:--:--\n",
      "     ---------- --------------------------- 286.7/999.0 kB 5.9 MB/s eta 0:00:01\n",
      "     --------------- ---------------------- 399.4/999.0 kB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 788.5/999.0 kB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 952.3/999.0 kB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 999.0/999.0 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting regex==2022.10.31 (from -r requirements.txt (line 60))\n",
      "  Downloading regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/267.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 267.7/267.7 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting requests==2.31.0 (from -r requirements.txt (line 61))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scikit-learn==1.2.0 (from -r requirements.txt (line 62))\n",
      "  Downloading scikit_learn-1.2.0-cp38-cp38-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/8.2 MB 6.5 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.5/8.2 MB 5.7 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.8/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.1/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.3/8.2 MB 5.6 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.6/8.2 MB 5.8 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.9/8.2 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.1/8.2 MB 5.8 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.1/8.2 MB 5.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.4/8.2 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 2.7/8.2 MB 5.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 3.1/8.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 3.4/8.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.8/8.2 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 4.2/8.2 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.6/8.2 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 5.0/8.2 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 5.3/8.2 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.6/8.2 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.1/8.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 6.4/8.2 MB 6.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.9/8.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 7.2/8.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.8/8.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.2/8.2 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting scipy==1.10.0 (from -r requirements.txt (line 63))\n",
      "  Downloading scipy-1.10.0-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.4/42.2 MB 8.9 MB/s eta 0:00:05\n",
      "      --------------------------------------- 0.8/42.2 MB 8.9 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.3/42.2 MB 9.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 1.7/42.2 MB 9.2 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 2.0/42.2 MB 8.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.3/42.2 MB 8.0 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 2.5/42.2 MB 7.5 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 2.7/42.2 MB 7.3 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 3.1/42.2 MB 7.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.2/42.2 MB 6.8 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.4/42.2 MB 6.5 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 3.5/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 3.8/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.1/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 4.3/42.2 MB 6.1 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 4.7/42.2 MB 6.2 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 5.0/42.2 MB 6.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.3/42.2 MB 6.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.5/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.9/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.0/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 6.6/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.0/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.2/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.7/42.2 MB 6.2 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.9/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.9/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.1/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.4/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 8.6/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 9.0/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 9.6/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.1/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.3/42.2 MB 6.1 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 10.5/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 10.9/42.2 MB 6.0 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 11.3/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 11.6/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.0/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.3/42.2 MB 5.9 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.5/42.2 MB 5.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 13.0/42.2 MB 6.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 13.5/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 13.6/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 13.9/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 14.2/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 14.6/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 14.8/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.1/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.5/42.2 MB 6.1 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.8/42.2 MB 6.2 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.2/42.2 MB 6.2 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.5/42.2 MB 6.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.8/42.2 MB 6.3 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 17.0/42.2 MB 6.3 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 17.3/42.2 MB 6.2 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 17.7/42.2 MB 6.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 18.1/42.2 MB 6.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 18.6/42.2 MB 6.9 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 19.0/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 19.3/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 19.8/42.2 MB 7.0 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 20.3/42.2 MB 7.1 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 20.6/42.2 MB 7.2 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 21.0/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 21.4/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 22.0/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.2/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.4/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.8/42.2 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 23.1/42.2 MB 7.3 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.2/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.5/42.2 MB 7.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.7/42.2 MB 7.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 24.0/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 24.2/42.2 MB 7.2 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.4/42.2 MB 7.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.5/42.2 MB 7.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.6/42.2 MB 6.7 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.7/42.2 MB 6.6 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.8/42.2 MB 6.5 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 24.9/42.2 MB 6.4 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 25.0/42.2 MB 6.3 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 25.1/42.2 MB 6.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.3/42.2 MB 6.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.5/42.2 MB 5.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.6/42.2 MB 5.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.8/42.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 25.9/42.2 MB 5.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.0/42.2 MB 5.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.3/42.2 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 26.3/42.2 MB 5.4 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 26.4/42.2 MB 5.2 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 26.7/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 27.0/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 27.1/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 27.4/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 27.7/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 28.0/42.2 MB 5.2 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 28.4/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 28.7/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.1/42.2 MB 5.1 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.4/42.2 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 29.5/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 30.0/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 30.5/42.2 MB 4.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 30.9/42.2 MB 4.6 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.0/42.2 MB 4.6 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.1/42.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.3/42.2 MB 4.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.6/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 31.9/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.1/42.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.4/42.2 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.7/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.1/42.2 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.5/42.2 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.0/42.2 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.3/42.2 MB 4.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 34.7/42.2 MB 4.7 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 34.9/42.2 MB 4.9 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.1/42.2 MB 4.9 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.3/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.5/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.6/42.2 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.8/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.1/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.2/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.4/42.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.6/42.2 MB 5.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.9/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.1/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.3/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.3/42.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.5/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.6/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.0/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.4/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.8/42.2 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.2/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.4/42.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.7/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 40.0/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.3/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.6/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.8/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.0/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.2/42.2 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.6/42.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.2 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.2/42.2 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.2/42.2 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.2/42.2 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Collecting soupsieve==2.3.2.post1 (from -r requirements.txt (line 65))\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: stack-data==0.6.2 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from -r requirements.txt (line 66)) (0.6.2)\n",
      "Collecting tenacity==8.1.0 (from -r requirements.txt (line 67))\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 68))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tiktoken==0.1.2 (from -r requirements.txt (line 69))\n",
      "  Downloading tiktoken-0.1.2-cp38-cp38-win_amd64.whl (575 kB)\n",
      "     ---------------------------------------- 0.0/575.5 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 61.4/575.5 kB 3.2 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 245.8/575.5 kB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 399.4/575.5 kB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 522.2/575.5 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 575.5/575.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tokenizers==0.13.2 (from -r requirements.txt (line 70))\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.3 MB 6.8 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.6/3.3 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.9/3.3 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.3 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.1/3.3 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.3/3.3 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.6/3.3 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 2.0/3.3 MB 3.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.4/3.3 MB 3.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.8/3.3 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.0/3.3 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.3/3.3 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.3/3.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting tornado==6.3.3 (from -r requirements.txt (line 71))\n",
      "  Downloading tornado-6.3.3-cp38-abi3-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 72))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Collecting traitlets==5.8.1 (from -r requirements.txt (line 73))\n",
      "  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 116.8/116.8 kB ? eta 0:00:00\n",
      "Collecting transformers==4.30.0 (from -r requirements.txt (line 74))\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 6.5 MB/s eta 0:00:00\n",
      "Collecting typing_extensions==4.4.0 (from -r requirements.txt (line 75))\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting urllib3==1.26.13 (from -r requirements.txt (line 76))\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     ---------------------------------------- 0.0/140.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 140.6/140.6 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting wcwidth==0.2.5 (from -r requirements.txt (line 77))\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting yarg==0.1.9 (from -r requirements.txt (line 78))\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Collecting yarl==1.8.2 (from -r requirements.txt (line 79))\n",
      "  Downloading yarl-1.8.2-cp38-cp38-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.9/56.9 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from ipython==8.10.0->-r requirements.txt (line 27)) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from jupyter_core==5.1.3->-r requirements.txt (line 31)) (227)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.30.0->-r requirements.txt (line 74))\n",
      "  Downloading safetensors-0.4.2-cp38-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.0.12->-r requirements.txt (line 24))\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (2.5.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\r2com\\anaconda3\\envs\\espnet2\\lib\\site-packages (from openai->-r requirements.txt (line 39)) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openai (from -r requirements.txt (line 39))\n",
      "  Using cached openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached openai-1.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: pip is still looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading openai-1.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.9-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.8-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.7-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai->-r requirements.txt (line 39))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting openai (from -r requirements.txt (line 39))\n",
      "  Downloading openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading openai-1.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.4-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->-r requirements.txt (line 41))\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading aiohttp-3.8.5-cp38-cp38-win_amd64.whl (327 kB)\n",
      "   ---------------------------------------- 0.0/327.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 327.9/327.9 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 158.3/158.3 kB 9.9 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading tornado-6.3.3-cp38-abi3-win_amd64.whl (429 kB)\n",
      "   ---------------------------------------- 0.0/429.2 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 368.6/429.2 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 368.6/429.2 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 429.2/429.2 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/7.2 MB 7.3 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.5/7.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/7.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.4/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.5/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.9/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.1/7.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.4/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.6/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.1/7.2 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.6/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.9/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.4/7.2 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.2 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.3/7.2 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.2/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.4/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 330.1/330.1 kB 10.3 MB/s eta 0:00:00\n",
      "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/77.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.0/77.0 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.8 MB 9.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/10.8 MB 10.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.2/10.8 MB 8.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/10.8 MB 7.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/10.8 MB 7.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.2/10.8 MB 6.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/10.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.3/10.8 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.8 MB 7.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.1/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.4/10.8 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.9/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.0/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/10.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.9/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.7/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/10.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.4/10.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.4/10.8 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 7.6 MB/s eta 0:00:00\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Downloading safetensors-0.4.2-cp38-none-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.7/268.7 kB 8.3 MB/s eta 0:00:00\n",
      "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: bs4, docopt\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1265 sha256=35d5cb9b550d8dc23db1ca1fd8fb2fa99a2019c0edc7ae71ed3caa14994071d7\n",
      "  Stored in directory: c:\\users\\r2com\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=0ed17b18f55f0ae7b0d69ccf7e0043378dd970e6de1f37f94a718fd7f71a1aed\n",
      "  Stored in directory: c:\\users\\r2com\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built bs4 docopt\n",
      "Installing collected packages: wcwidth, tokenizers, pytz, ptyprocess, executing, docopt, appnope, urllib3, tzdata, typing_extensions, traitlets, tqdm, tornado, threadpoolctl, tenacity, soupsieve, safetensors, regex, pyzmq, PyYAML, pyparsing, Pygments, pycryptodomex, psutil, prompt-toolkit, platformdirs, Pillow, pexpect, packaging, numpy, nest-asyncio, multidict, lxml, kiwisolver, joblib, jedi, idna, fsspec, frozenlist, fonttools, filelock, entrypoints, debugpy, cycler, charset-normalizer, certifi, attrs, async-timeout, asttokens, yarl, scipy, requests, plotly, pandas, jupyter_core, contourpy, comm, blobfile, beautifulsoup4, aiosignal, yarg, tiktoken, scikit-learn, matplotlib, jupyter_client, ipython, huggingface-hub, bs4, aiohttp, transformers, pipreqs, openai, ipykernel\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.13\n",
      "    Uninstalling wcwidth-0.2.13:\n",
      "      Successfully uninstalled wcwidth-0.2.13\n",
      "  Attempting uninstall: executing\n",
      "    Found existing installation: executing 2.0.1\n",
      "    Uninstalling executing-2.0.1:\n",
      "      Successfully uninstalled executing-2.0.1\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.14.1\n",
      "    Uninstalling traitlets-5.14.1:\n",
      "      Successfully uninstalled traitlets-5.14.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.2\n",
      "    Uninstalling tornado-6.2:\n",
      "      Successfully uninstalled tornado-6.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 25.1.2\n",
      "    Uninstalling pyzmq-25.1.2:\n",
      "      Successfully uninstalled pyzmq-25.1.2\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.17.2\n",
      "    Uninstalling Pygments-2.17.2:\n",
      "      Successfully uninstalled Pygments-2.17.2\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.0\n",
      "    Uninstalling psutil-5.9.0:\n",
      "      Successfully uninstalled psutil-5.9.0\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.42\n",
      "    Uninstalling prompt-toolkit-3.0.42:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.42\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest_asyncio 1.6.0\n",
      "    Uninstalling nest_asyncio-1.6.0:\n",
      "      Successfully uninstalled nest_asyncio-1.6.0\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.1.0\n",
      "    Uninstalling lxml-5.1.0:\n",
      "      Successfully uninstalled lxml-5.1.0\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.3.2\n",
      "    Uninstalling joblib-1.3.2:\n",
      "      Successfully uninstalled joblib-1.3.2\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.1\n",
      "    Uninstalling jedi-0.19.1:\n",
      "      Successfully uninstalled jedi-0.19.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.47.2\n",
      "    Uninstalling fonttools-4.47.2:\n",
      "      Successfully uninstalled fonttools-4.47.2\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.6.7\n",
      "    Uninstalling debugpy-1.6.7:\n",
      "      Successfully uninstalled debugpy-1.6.7\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: asttokens\n",
      "    Found existing installation: asttokens 2.4.1\n",
      "    Uninstalling asttokens-2.4.1:\n",
      "      Successfully uninstalled asttokens-2.4.1\n",
      "  Attempting uninstall: jupyter_core\n",
      "    Found existing installation: jupyter_core 4.12.0\n",
      "    Uninstalling jupyter_core-4.12.0:\n",
      "      Successfully uninstalled jupyter_core-4.12.0\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.1.1\n",
      "    Uninstalling contourpy-1.1.1:\n",
      "      Successfully uninstalled contourpy-1.1.1\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.2.1\n",
      "    Uninstalling comm-0.2.1:\n",
      "      Successfully uninstalled comm-0.2.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.4\n",
      "    Uninstalling matplotlib-3.7.4:\n",
      "      Successfully uninstalled matplotlib-3.7.4\n",
      "  Attempting uninstall: jupyter_client\n",
      "    Found existing installation: jupyter_client 8.6.0\n",
      "    Uninstalling jupyter_client-8.6.0:\n",
      "      Successfully uninstalled jupyter_client-8.6.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.12.0\n",
      "    Uninstalling ipython-8.12.0:\n",
      "      Successfully uninstalled ipython-8.12.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.10.0\n",
      "    Uninstalling openai-1.10.0:\n",
      "      Successfully uninstalled openai-1.10.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.29.0\n",
      "    Uninstalling ipykernel-6.29.0:\n",
      "      Successfully uninstalled ipykernel-6.29.0\n",
      "Successfully installed Pillow-9.4.0 PyYAML-6.0 Pygments-2.15.0 aiohttp-3.8.5 aiosignal-1.3.1 appnope-0.1.3 asttokens-2.2.1 async-timeout-4.0.2 attrs-22.2.0 beautifulsoup4-4.11.1 blobfile-2.0.1 bs4-0.0.1 certifi-2023.7.22 charset-normalizer-2.1.1 comm-0.1.2 contourpy-1.0.7 cycler-0.11.0 debugpy-1.6.5 docopt-0.6.2 entrypoints-0.4 executing-1.2.0 filelock-3.9.0 fonttools-4.38.0 frozenlist-1.3.3 fsspec-2023.12.2 huggingface-hub-0.20.3 idna-3.4 ipykernel-6.20.1 ipython-8.10.0 jedi-0.18.2 joblib-1.2.0 jupyter_client-7.4.8 jupyter_core-5.1.3 kiwisolver-1.4.4 lxml-4.9.2 matplotlib-3.6.3 multidict-6.0.4 nest-asyncio-1.5.6 numpy-1.24.1 openai-0.28.1 packaging-23.0 pandas-2.0.3 pexpect-4.8.0 pipreqs-0.4.12 platformdirs-2.6.2 plotly-5.12.0 prompt-toolkit-3.0.36 psutil-5.9.4 ptyprocess-0.7.0 pycryptodomex-3.17 pyparsing-3.0.9 pytz-2022.7.1 pyzmq-24.0.1 regex-2022.10.31 requests-2.31.0 safetensors-0.4.2 scikit-learn-1.2.0 scipy-1.10.0 soupsieve-2.3.2.post1 tenacity-8.1.0 threadpoolctl-3.1.0 tiktoken-0.1.2 tokenizers-0.13.2 tornado-6.3.3 tqdm-4.64.1 traitlets-5.8.1 transformers-4.30.0 typing_extensions-4.4.0 tzdata-2023.4 urllib3-1.26.13 wcwidth-0.2.5 yarg-0.1.9 yarl-1.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~ornado'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~mq'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~sutil'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\r2com\\anaconda3\\envs\\espnet2\\Lib\\site-packages\\~ebugpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.5.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.4.0 which is incompatible.\n",
      "pydantic-core 2.14.6 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai   .         .     ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "# Define root domain to crawl\n",
    "domain = \"openai.com\"\n",
    "full_url = \"https://openai.com/\"\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding='utf-8') as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the text but remove the tags\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # Otherwise, write the text to the file in the text directory\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openai.com/\n",
      "https://openai.com/research\n",
      "https://openai.com/research/confidence-building-measures-for-artificial-intelligence\n",
      "https://openai.com/research/confidence-building-measures-for-artificial-intelligence#content\n",
      "https://openai.com/research?contentTypes=publication\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'text/openai.com/openai.com_research?contentTypes=publication.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#    20       .\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#    3    .\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(url) \u001b[38;5;66;03m# for debugging and to see the progress\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Save text from the url to a <url>.txt file\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlocal_domain\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Get the text from the URL using BeautifulSoup\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Get the text but remove the tags\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Documents\\GitHub\\sesac-project\\.env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'text/openai.com/openai.com_research?contentTypes=publication.txt'"
     ]
    }
   ],
   "source": [
    "#    20       .\n",
    "#    3    .\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2com\\AppData\\Local\\Temp\\ipykernel_16612\\2091664849.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xc2 in position 2001: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Open the file and read the text\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 11\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend((file[\u001b[38;5;241m11\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#update\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), text))\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xc2 in position 2001: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#    \n",
    "texts=[]\n",
    "\n",
    "#       \n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "#      \n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#       text  .\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mfname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m remove_newlines(df\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/scraped.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#       text  .\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##         . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "#  cl100k_base .    .\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "#     . apply  !\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1239.000000\n",
       "mean     2116.348668\n",
       "std      1438.308043\n",
       "min         3.000000\n",
       "25%      1025.000000\n",
       "50%      1353.000000\n",
       "75%      3886.000000\n",
       "max      8258.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#   \n",
    "df.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       . ,       .\n",
    "## text-ada-002 ,    4095."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2000\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    #   \n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    #      .\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        #           ,           .\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened = []\n",
    "#   \n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'research?authors=henk tillman.  Research index    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLE 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALLE 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Filter selectionsTopicsAdversarial examples\\xa0 (4)Audio generation\\xa0 (2)Community\\xa0 (12)Compute\\xa0 (8)Computer vision\\xa0 (9)Contrastive learning\\xa0 (2)Domain randomization\\xa0 (6)Dota 2\\xa0 (7)Environments\\xa0 (10)Exploration\\xa0 (4)Games\\xa0 (19)Generative models\\xa0 (19)Human feedback\\xa0 (11)Image generation\\xa0 (5)Interpretability\\xa0 (4)Language\\xa0 (30)Memory\\xa0 (1)Meta-learning\\xa0 (9)Multi-agent\\xa0 (7)Open source\\xa0 (20)Policy optimization\\xa0 (5)Procedural generation\\xa0 (3)Reasoning\\xa0 (3)Reinforcement learning\\xa0 (56)Representation learning\\xa0 (8)Research\\xa0 (2)Responsible AI\\xa0 (18)Robotics\\xa0 (18)Robustness\\xa0 (8)Safety & Alignment\\xa0 (35)Scaling properties\\xa0 (3)Self-play\\xa0 (9)Sim-to-real\\xa0 (5)Software engineering\\xa0 (21)Sparsity\\xa0 (2)Speech recognition\\xa0 (1)Summarization\\xa0 (4)Supervised learning\\xa0 (2)Transfer learning\\xa0 (7)Transformers\\xa0 (9)Unsupervised learning\\xa0 (6)ModelsCLIP\\xa0 (4)Dactyl\\xa0 (2)DALLE\\xa0 (1)DALLE 2\\xa0 (2)DALLE 3\\xa0 (1)Glow\\xa0 (1)GPT\\xa0 (1)GPT-2\\xa0 (5)GPT-3\\xa0 (6)GPT-4\\xa0 (3)Jukebox\\xa0 (1)MuseNet\\xa0 (1)OpenAI Codex\\xa0 (3)OpenAI Five\\xa0 (7)Point-E\\xa0 (1)Whisper\\xa0 (1)TypesConclusion\\xa0 (14)Milestone\\xa0 (33)Publication\\xa0 (132)Release\\xa0 (52)AuthorsMart\\xadn Abadi\\xa0 (1)Pieter Abbeel\\xa0 (35)Joshua Achiam\\xa0 (8)Steven Adler\\xa0 (2)Sandhini Agarwal\\xa0 (5)Lama Ahmad\\xa0 (1)Ilge Akkaya\\xa0 (2)Maruan Al-Shedivat\\xa0 (4)Dario Amodei\\xa0 (16)Daniella Amodei\\xa0 (1)Daniela Amodei\\xa0 (1)Marcin Andrychowicz\\xa0 (12)Leopold Aschenbrenner\\xa0 (1)Tasmin Asfour\\xa0 (1)Amanda Askell\\xa0 (4)Anish Athalye\\xa0 (1)Igor Babuschkin\\xa0 (1)Bowen Baker\\xa0 (6)Suchir Balaji\\xa0 (2)Trapit Bansal\\xa0 (2)Yamini Bansal\\xa0 (1)Boaz Barak\\xa0 (1)Elizabeth Barnes\\xa0 (1)Ben Barry\\xa0 (1)Peter L. Bartlett\\xa0 (1)Mohammad Bavarian\\xa0 (2)Alexandre M Bayen\\xa0 (1)Christopher Berner\\xa0 (4)Jesse Bettencourt\\xa0 (1)Alex Beutel\\xa0 (1)Lukas Biewald\\xa0 (1)Steven Bills\\xa0 (1)Xue Bin Peng\\xa0 (2)Trevor Blackwell\\xa0 (1)Greg Brockman\\xa0 (14)Tom Brown\\xa0 (6)Miles Brundage\\xa0 (8)Yura Burda\\xa0 (7)Collin Burns\\xa0 (1)Nick Cammarata\\xa0 (2)Rosie Campbell\\xa0 (1)Andrew N. Carr\\xa0 (1)Shan Carter\\xa0 (2)Brooke Chan\\xa0 (3)Fotios Chantzis\\xa0 (1)Peter Chen\\xa0 (3)Richard Chen\\xa0 (6)Xi Chen\\xa0 (7)Mark Chen\\xa0 (7)Ricky T. Q. Chen\\xa0 (1)Benjamin Chess\\xa0 (3)Vicki Cheung\\xa0 (3)Rewon Child\\xa0 (4)Maciek Chociej\\xa0 (4)Paul Christiano\\xa0 (9)Casey Chu\\xa0 (1)Jack Clark\\xa0 (15)Jeff Clune\\xa0 (1)Karl Cobbe\\xa0 (4)Taco Cohen\\xa0 (1)Dave Cummings\\xa0 (1)Andrew M. Dai\\xa0 (1)Trevor Darrell\\xa0 (1)Przemys\\x82aw D\\x99biak\\xa0 (2)Akshay Degwekar\\xa0 (1)Christy Dennison\\xa0 (3)Filip De Turck\\xa0 (1)Prafulla Dhariwal\\xa0 (9)Yilun Du\\xa0 (2)Yan Duan\\xa0 (12)David Duvenaud\\xa0 (1)Harri Edwards\\xa0 (6)Alexei A. Efros\\xa0 (1)Tyna Eloundou\\xa0 (3)\\x9alfar Erlingsson\\xa0 (1)Owain Evans\\xa0 (2)David Farhi\\xa0 (2)Chelsea Finn\\xa0 (1)Quirin Fischer\\xa0 (2)Carlos Florensa\\xa0 (1)Jakob Foerster\\xa0 (3)Rachel Fong\\xa0 (3)Davis Foote\\xa0 (1)Kevin Frans\\xa0 (1)Deep Ganguli\\xa0 (1)Leo Gao\\xa0 (4)Jon Gauthier\\xa0 (1)Gabriel Goh\\xa0 (3)Ian Goodfellow\\xa0 (5)Jonathan Gordon\\xa0 (1)Will Grathwohl\\xa0 (1)Scott Gray\\xa0 (8)Roger Grosse\\xa0 (1)Aditya Grover\\xa0 (1)Jayesh K.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shortened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvQklEQVR4nO3de3TU5YHG8ScJkwkBJjFgEiJJxEu5CAiChKnWKoQETC1KzlY0q9FlYUuDVdMiTRchgBXErri6EeweBHuU2rJbcUUEAgrUEm6pVC42C5QaKyTpSpMAkclA3v3DzaxjuGTCTPPO+P2ck0Pm93vnnffJLzPzMJdMlDHGCAAAwCLRnb0AAACAL6OgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACs06WzF9ARLS0tOnr0qHr06KGoqKjOXg4AAGgHY4xOnDihtLQ0RUdf+DGSsCwoR48eVXp6emcvAwAAdMDHH3+sPn36XHBMWBaUHj16SPo8oMvlCtq8Xq9XGzZsUE5OjhwOR9DmtUWk55MiP2Ok55MiPyP5wl+kZwxlvsbGRqWnp/vuxy8kLAtK69M6Lpcr6AUlPj5eLpcrYn/pIjmfFPkZIz2fFPkZyRf+Ij3j3yJfe16ewYtkAQCAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKzTpbMXAACR4MofvdWucc4Yo0UjpUGl6+U5e/GPnA+lPy3M69TLBy6ER1AAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUCLiiffPKJ/v7v/149e/ZU165dNXjwYO3evdu33xij2bNnq3fv3uratauys7N18OBBvzmOHz+ugoICuVwuJSYmavLkyTp58uSlpwEAABEhoILy17/+VTfddJMcDofefvttHThwQP/yL/+iyy67zDdm0aJFeu6557R06VLt2LFD3bp1U25urk6fPu0bU1BQoP3796u8vFxr1qzR1q1bNXXq1OClAgAAYS2gz+J56qmnlJ6eruXLl/u29e3b1/e9MUbPPvusZs2apQkTJkiSfv7znyslJUWrV6/WpEmT9OGHH2rdunXatWuXRowYIUl6/vnndfvtt+unP/2p0tLSgpELAACEsYAKyn/9138pNzdXf/d3f6ctW7boiiuu0Pe+9z1NmTJFknTkyBHV1NQoOzvbd56EhARlZWWpoqJCkyZNUkVFhRITE33lRJKys7MVHR2tHTt26K677mpzuR6PRx6Px3e6sbFRkuT1euX1egNLfAGtcwVzTptEej4p8jNGej4pfDM6Y0z7xkUbv387Uyh+xuF6/AIR6RlDmS+QOQMqKH/84x+1ZMkSFRcX68c//rF27dql73//+4qNjVVhYaFqamokSSkpKX7nS0lJ8e2rqalRcnKy/yK6dFFSUpJvzJctWLBAc+fObbN9w4YNio+PDyRCu5SXlwd9TptEej4p8jNGej4p/DIuGhnY+PkjWkKzkACsXbs2ZHOH2/HriEjPGIp8TU1N7R4bUEFpaWnRiBEj9OSTT0qShg0bpn379mnp0qUqLCwMbJUBKCkpUXFxse90Y2Oj0tPTlZOTI5fLFbTL8Xq9Ki8v19ixY+VwOII2ry0iPZ8U+RkjPZ8UvhkHla5v1zhntNH8ES16fHe0PC1RIV7Vhe0rzQ36nOF6/AIR6RlDma/1GZD2CKig9O7dWwMHDvTbNmDAAP3nf/6nJCk1NVWSVFtbq969e/vG1NbWaujQob4xdXV1fnOcOXNGx48f953/y5xOp5xOZ5vtDocjJL8coZrXFpGeT4r8jJGeTwq/jJ6zgZUNT0tUwOcJtlD+fMPt+HVEpGcMRb5A5gvoXTw33XSTqqqq/Lb993//tzIzMyV9/oLZ1NRUbdq0ybe/sbFRO3bskNvtliS53W7V19ersrLSN+add95RS0uLsrKyAlkOAACIUAE9gvLoo4/q61//up588kl95zvf0c6dO/Wzn/1MP/vZzyRJUVFReuSRR/TEE0/o2muvVd++ffX4448rLS1Nd955p6TPH3EZN26cpkyZoqVLl8rr9Wr69OmaNGkS7+ABAACSAiwoN954o15//XWVlJRo3rx56tu3r5599lkVFBT4xjz22GM6deqUpk6dqvr6et18881at26d4uLifGNeffVVTZ8+XWPGjFF0dLTy8/P13HPPBS8VAAAIawEVFEn61re+pW9961vn3R8VFaV58+Zp3rx55x2TlJSklStXBnrRAADgK4LP4gEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALBOQAWltLRUUVFRfl/9+/f37T99+rSKiorUs2dPde/eXfn5+aqtrfWbo7q6Wnl5eYqPj1dycrJmzJihM2fOBCcNAACICF0CPcN1112njRs3/v8EXf5/ikcffVRvvfWWVq1apYSEBE2fPl0TJ07Ub3/7W0nS2bNnlZeXp9TUVG3btk3Hjh3T/fffL4fDoSeffDIIcQAAQCQIuKB06dJFqampbbY3NDRo2bJlWrlypUaPHi1JWr58uQYMGKDt27dr1KhR2rBhgw4cOKCNGzcqJSVFQ4cO1fz58zVz5kyVlpYqNjb20hMBAICwF3BBOXjwoNLS0hQXFye3260FCxYoIyNDlZWV8nq9ys7O9o3t37+/MjIyVFFRoVGjRqmiokKDBw9WSkqKb0xubq6mTZum/fv3a9iwYee8TI/HI4/H4zvd2NgoSfJ6vfJ6vYFGOK/WuYI5p00iPZ8U+RkjPZ8UvhmdMaZ946KN37+dKRQ/43A9foGI9IyhzBfInFHGmHZfS95++22dPHlS/fr107FjxzR37lx98skn2rdvn9588009+OCDfkVCkkaOHKnbbrtNTz31lKZOnaqPPvpI69ev9+1vampSt27dtHbtWo0fP/6cl1taWqq5c+e22b5y5UrFx8e3d/kAAKATNTU16d5771VDQ4NcLtcFxwb0CMoXC8SQIUOUlZWlzMxM/epXv1LXrl07ttp2KCkpUXFxse90Y2Oj0tPTlZOTc9GAgfB6vSovL9fYsWPlcDiCNq8tIj2fFPkZIz2fFL4ZB5Wuv/ggff7IyfwRLXp8d7Q8LVEhXtWF7SvNDfqc4Xr8AhHpGUOZr/UZkPYI+CmeL0pMTNTXvvY1HTp0SGPHjlVzc7Pq6+uVmJjoG1NbW+t7zUpqaqp27tzpN0fru3zO9bqWVk6nU06ns812h8MRkl+OUM1ri0jPJ0V+xkjPJ4VfRs/ZwMqGpyUq4PMEWyh/vuF2/Doi0jOGIl8g813S30E5efKkDh8+rN69e2v48OFyOBzatGmTb39VVZWqq6vldrslSW63W3v37lVdXZ1vTHl5uVwulwYOHHgpSwEAABEkoEdQfvjDH+qOO+5QZmamjh49qjlz5igmJkb33HOPEhISNHnyZBUXFyspKUkul0sPPfSQ3G63Ro0aJUnKycnRwIEDdd9992nRokWqqanRrFmzVFRUdM5HSAAAwFdTQAXlz3/+s+655x59+umnuvzyy3XzzTdr+/btuvzyyyVJixcvVnR0tPLz8+XxeJSbm6sXXnjBd/6YmBitWbNG06ZNk9vtVrdu3VRYWKh58+YFNxUAAAhrARWU11577YL74+LiVFZWprKysvOOyczM1Nq1awO5WAAA8BXDZ/EAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwziUVlIULFyoqKkqPPPKIb9vp06dVVFSknj17qnv37srPz1dtba3f+aqrq5WXl6f4+HglJydrxowZOnPmzKUsBQAARJAOF5Rdu3bpxRdf1JAhQ/y2P/roo3rzzTe1atUqbdmyRUePHtXEiRN9+8+ePau8vDw1Nzdr27Ztevnll7VixQrNnj274ykAAEBE6VBBOXnypAoKCvTv//7vuuyyy3zbGxoatGzZMj3zzDMaPXq0hg8fruXLl2vbtm3avn27JGnDhg06cOCAXnnlFQ0dOlTjx4/X/PnzVVZWpubm5uCkAgAAYa1LR85UVFSkvLw8ZWdn64knnvBtr6yslNfrVXZ2tm9b//79lZGRoYqKCo0aNUoVFRUaPHiwUlJSfGNyc3M1bdo07d+/X8OGDWtzeR6PRx6Px3e6sbFRkuT1euX1ejsS4Zxa5wrmnDaJ9HxS5GeM9HxS+GZ0xpj2jYs2fv92plD8jMP1+AUi0jOGMl8gcwZcUF577TX97ne/065du9rsq6mpUWxsrBITE/22p6SkqKamxjfmi+WkdX/rvnNZsGCB5s6d22b7hg0bFB8fH2iEiyovLw/6nDaJ9HxS5GeM9HxS+GVcNDKw8fNHtIRmIQFYu3ZtyOYOt+PXEZGeMRT5mpqa2j02oILy8ccf6+GHH1Z5ebni4uICXlhHlZSUqLi42He6sbFR6enpysnJkcvlCtrleL1elZeXa+zYsXI4HEGb1xaRnk+K/IyRnk8K34yDSte3a5wz2mj+iBY9vjtanpaoEK/qwvaV5gZ9znA9foGI9IyhzNf6DEh7BFRQKisrVVdXpxtuuMG37ezZs9q6dav+7d/+TevXr1dzc7Pq6+v9HkWpra1VamqqJCk1NVU7d+70m7f1XT6tY77M6XTK6XS22e5wOELyyxGqeW0R6fmkyM8Y6fmk8MvoORtY2fC0RAV8nmAL5c833I5fR0R6xlDkC2S+gF4kO2bMGO3du1d79uzxfY0YMUIFBQW+7x0OhzZt2uQ7T1VVlaqrq+V2uyVJbrdbe/fuVV1dnW9MeXm5XC6XBg4cGMhyAABAhAroEZQePXpo0KBBftu6deumnj17+rZPnjxZxcXFSkpKksvl0kMPPSS3261Ro0ZJknJycjRw4EDdd999WrRokWpqajRr1iwVFRWd81ESAADw1dOhd/FcyOLFixUdHa38/Hx5PB7l5ubqhRde8O2PiYnRmjVrNG3aNLndbnXr1k2FhYWaN29esJcCAADC1CUXlM2bN/udjouLU1lZmcrKys57nszMzJC+ehwAAIQ3PosHAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdbp09gIAAIhkV/7orc5eQkCcMUaLRnb2KngEBQAAWIiCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYJ2ACsqSJUs0ZMgQuVwuuVwuud1uvf322779p0+fVlFRkXr27Knu3bsrPz9ftbW1fnNUV1crLy9P8fHxSk5O1owZM3TmzJngpAEAABEhoILSp08fLVy4UJWVldq9e7dGjx6tCRMmaP/+/ZKkRx99VG+++aZWrVqlLVu26OjRo5o4caLv/GfPnlVeXp6am5u1bds2vfzyy1qxYoVmz54d3FQAACCsdQlk8B133OF3+ic/+YmWLFmi7du3q0+fPlq2bJlWrlyp0aNHS5KWL1+uAQMGaPv27Ro1apQ2bNigAwcOaOPGjUpJSdHQoUM1f/58zZw5U6WlpYqNjQ1eMgAAELYCKihfdPbsWa1atUqnTp2S2+1WZWWlvF6vsrOzfWP69++vjIwMVVRUaNSoUaqoqNDgwYOVkpLiG5Obm6tp06Zp//79GjZs2Dkvy+PxyOPx+E43NjZKkrxer7xeb0cjtNE6VzDntEmk55MiP2Ok55PCN6MzxrRvXLTx+7czheJnHK7HLxCBZmzv74YtWn83Q/n70R5RxpiAfnJ79+6V2+3W6dOn1b17d61cuVK33367Vq5cqQcffNCvSEjSyJEjddttt+mpp57S1KlT9dFHH2n9+vW+/U1NTerWrZvWrl2r8ePHn/MyS0tLNXfu3DbbV65cqfj4+ECWDwAAOklTU5PuvfdeNTQ0yOVyXXBswI+g9OvXT3v27FFDQ4P+4z/+Q4WFhdqyZUuHF9seJSUlKi4u9p1ubGxUenq6cnJyLhowEF6vV+Xl5Ro7dqwcDkfQ5rVFpOeTIj9jpOeTwjfjoNL1Fx+kz/93On9Eix7fHS1PS1SIV3Vh+0pzgz5nuB6/QASasb2/G7Zo/R0NxTFsfQakPQIuKLGxsbrmmmskScOHD9euXbv0r//6r7r77rvV3Nys+vp6JSYm+sbX1tYqNTVVkpSamqqdO3f6zdf6Lp/WMefidDrldDrbbHc4HCG5AoRqXltEej4p8jNGej4p/DJ6zgZWNjwtUQGfJ9hC+fMNt+PXEe3N2NnHuaNCcQwDme+S/w5KS0uLPB6Phg8fLofDoU2bNvn2VVVVqbq6Wm63W5Lkdru1d+9e1dXV+caUl5fL5XJp4MCBl7oUAAAQIQJ6BKWkpETjx49XRkaGTpw4oZUrV2rz5s1av369EhISNHnyZBUXFyspKUkul0sPPfSQ3G63Ro0aJUnKycnRwIEDdd9992nRokWqqanRrFmzVFRUdM5HSAAAoXPlj94K+pzOGKNFIz9/WiMUjxz8aWFe0OeEnQIqKHV1dbr//vt17NgxJSQkaMiQIVq/fr3Gjh0rSVq8eLGio6OVn58vj8ej3NxcvfDCC77zx8TEaM2aNZo2bZrcbre6deumwsJCzZs3L7ipAABAWAuooCxbtuyC++Pi4lRWVqaysrLzjsnMzNTatWsDuVgAAPAVw2fxAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwTpfOXgCA0LryR2919hIC4owxWjSys1cBoLPxCAoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArMNn8QCw0qDS9fKcjersZQDoJDyCAgAArENBAQAA1qGgAAAA6wRUUBYsWKAbb7xRPXr0UHJysu68805VVVX5jTl9+rSKiorUs2dPde/eXfn5+aqtrfUbU11drby8PMXHxys5OVkzZszQmTNnLj0NAACICAEVlC1btqioqEjbt29XeXm5vF6vcnJydOrUKd+YRx99VG+++aZWrVqlLVu26OjRo5o4caJv/9mzZ5WXl6fm5mZt27ZNL7/8slasWKHZs2cHLxUAAAhrAb2LZ926dX6nV6xYoeTkZFVWVuqWW25RQ0ODli1bppUrV2r06NGSpOXLl2vAgAHavn27Ro0apQ0bNujAgQPauHGjUlJSNHToUM2fP18zZ85UaWmpYmNjg5cOAACEpUt6DUpDQ4MkKSkpSZJUWVkpr9er7Oxs35j+/fsrIyNDFRUVkqSKigoNHjxYKSkpvjG5ublqbGzU/v37L2U5AAAgQnT476C0tLTokUce0U033aRBgwZJkmpqahQbG6vExES/sSkpKaqpqfGN+WI5ad3fuu9cPB6PPB6P73RjY6Mkyev1yuv1djRCG61zBXNOm0R6PinyM3YknzPGhGo5IeGMNn7/RhryXRobrtuBXg/D9ToYip91IHN2uKAUFRVp3759eu+99zo6RbstWLBAc+fObbN9w4YNio+PD/rllZeXB31Om0R6PinyMwaSb9HIEC4khOaPaOnsJYQU+Tpm7dq1IZm3I9p7PQzX62AobkebmpraPbZDBWX69Olas2aNtm7dqj59+vi2p6amqrm5WfX19X6PotTW1io1NdU3ZufOnX7ztb7Lp3XMl5WUlKi4uNh3urGxUenp6crJyZHL5epIhHPyer0qLy/X2LFj5XA4gjavLSI9nxT5GTuSb1Dp+hCvKric0UbzR7To8d3R8rRE3l+SJd+l2VeaG/Q5AxXo9TBcr4OhuB1tfQakPQIqKMYYPfTQQ3r99de1efNm9e3b12//8OHD5XA4tGnTJuXn50uSqqqqVF1dLbfbLUlyu936yU9+orq6OiUnJ0v6vKW5XC4NHDjwnJfrdDrldDrbbHc4HCG5EwrVvLaI9HxS5GcMJF+4/rl4T0tU2K69PcjXMTZdr9t7PQzX4xyK29FA5guooBQVFWnlypV644031KNHD99rRhISEtS1a1clJCRo8uTJKi4uVlJSklwulx566CG53W6NGjVKkpSTk6OBAwfqvvvu06JFi1RTU6NZs2apqKjonCUEAAB89QRUUJYsWSJJuvXWW/22L1++XA888IAkafHixYqOjlZ+fr48Ho9yc3P1wgsv+MbGxMRozZo1mjZtmtxut7p166bCwkLNmzfv0pIAAICIEfBTPBcTFxensrIylZWVnXdMZmamVS90AgAAduGzeAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKwTcEHZunWr7rjjDqWlpSkqKkqrV6/222+M0ezZs9W7d2917dpV2dnZOnjwoN+Y48ePq6CgQC6XS4mJiZo8ebJOnjx5SUEAAEDkCLignDp1Stdff73KysrOuX/RokV67rnntHTpUu3YsUPdunVTbm6uTp8+7RtTUFCg/fv3q7y8XGvWrNHWrVs1derUjqcAAAARpUugZxg/frzGjx9/zn3GGD377LOaNWuWJkyYIEn6+c9/rpSUFK1evVqTJk3Shx9+qHXr1mnXrl0aMWKEJOn555/X7bffrp/+9KdKS0u7hDgAACASBFxQLuTIkSOqqalRdna2b1tCQoKysrJUUVGhSZMmqaKiQomJib5yIknZ2dmKjo7Wjh07dNddd7WZ1+PxyOPx+E43NjZKkrxer7xeb9DW3zpXMOe0SaTnkyI/Y0fyOWNMqJYTEs5o4/dvpCHfpbHhuh3o9TBcr4Oh+FkHMmdQC0pNTY0kKSUlxW97SkqKb19NTY2Sk5P9F9Gli5KSknxjvmzBggWaO3dum+0bNmxQfHx8MJbup7y8POhz2iTS80mRnzGQfItGhnAhITR/REtnLyGkyNcxa9euDcm8HdHe62G4XgdDcTva1NTU7rFBLSihUlJSouLiYt/pxsZGpaenKycnRy6XK2iX4/V6VV5errFjx8rhcARtXltEej4p8jN2JN+g0vUhXlVwOaON5o9o0eO7o+Vpiers5QQd+S7NvtLcoM8ZqECvh+F6HQzF7WjrMyDtEdSCkpqaKkmqra1V7969fdtra2s1dOhQ35i6ujq/8505c0bHjx/3nf/LnE6nnE5nm+0OhyMkd0KhmtcWkZ5PivyMgeTznA3PO0FPS1TYrr09yNcxNl2v23s9DNfjHIrb0UDmC+rfQenbt69SU1O1adMm37bGxkbt2LFDbrdbkuR2u1VfX6/KykrfmHfeeUctLS3KysoK5nIAAECYCvgRlJMnT+rQoUO+00eOHNGePXuUlJSkjIwMPfLII3riiSd07bXXqm/fvnr88ceVlpamO++8U5I0YMAAjRs3TlOmTNHSpUvl9Xo1ffp0TZo0iXfwAAAASR0oKLt379Ztt93mO9362pDCwkKtWLFCjz32mE6dOqWpU6eqvr5eN998s9atW6e4uDjfeV599VVNnz5dY8aMUXR0tPLz8/Xcc88FIQ4AAIgEAReUW2+9Vcac/y1TUVFRmjdvnubNm3feMUlJSVq5cmWgFw0AAL4i+CweAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALBOwH/qHvgqu/JHb3Xq5TtjjBaNlAaVrg/bj3AHgPbgERQAAGAdCgoAALAOT/Gg04Ti6RKeAgGAyEBBAQCEjc5+HZjEf4T+VniKBwAAWIeCAgAArENBAQAA1qGgAAAA6/Ai2XOI1Bc+8cIuAEC44BEUAABgHQoKAACwDgUFAABYh4ICAACsQ0EBAADWoaAAAADrUFAAAIB1KCgAAMA6FBQAAGAdCgoAALAOBQUAAFiHggIAAKxDQQEAANahoAAAAOtQUAAAgHUoKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgnU4tKGVlZbryyisVFxenrKws7dy5szOXAwAALNFpBeWXv/yliouLNWfOHP3ud7/T9ddfr9zcXNXV1XXWkgAAgCU6raA888wzmjJlih588EENHDhQS5cuVXx8vF566aXOWhIAALBEl8640ObmZlVWVqqkpMS3LTo6WtnZ2aqoqGgz3uPxyOPx+E43NDRIko4fPy6v1xu0dXm9XjU1NamLN1pnW6KCNq8turQYNTW1RGw+KfIzRno+KfIzki/8RXrG1nyffvqpHA5HUOc+ceKEJMkYc/HBphN88sknRpLZtm2b3/YZM2aYkSNHthk/Z84cI4kvvvjiiy+++IqAr48//viiXaFTHkEJVElJiYqLi32nW1padPz4cfXs2VNRUcFrr42NjUpPT9fHH38sl8sVtHltEen5pMjPGOn5pMjPSL7wF+kZQ5nPGKMTJ04oLS3tomM7paD06tVLMTExqq2t9dteW1ur1NTUNuOdTqecTqfftsTExJCtz+VyReQvXatIzydFfsZIzydFfkbyhb9IzxiqfAkJCe0a1ykvko2NjdXw4cO1adMm37aWlhZt2rRJbre7M5YEAAAs0mlP8RQXF6uwsFAjRozQyJEj9eyzz+rUqVN68MEHO2tJAADAEp1WUO6++2795S9/0ezZs1VTU6OhQ4dq3bp1SklJ6awlyel0as6cOW2eTooUkZ5PivyMkZ5PivyM5At/kZ7RlnxRxrTnvT4AAAB/O3wWDwAAsA4FBQAAWIeCAgAArENBAQAA1qGg/J+ysjJdeeWViouLU1ZWlnbu3NnZS2qXBQsW6MYbb1SPHj2UnJysO++8U1VVVX5jbr31VkVFRfl9ffe73/UbU11drby8PMXHxys5OVkzZszQmTNn/pZRzqu0tLTN+vv37+/bf/r0aRUVFalnz57q3r278vPz2/wRQJvzXXnllW3yRUVFqaioSFJ4Hr+tW7fqjjvuUFpamqKiorR69Wq//cYYzZ49W71791bXrl2VnZ2tgwcP+o05fvy4CgoK5HK5lJiYqMmTJ+vkyZN+Yz744AN94xvfUFxcnNLT07Vo0aJQR5N04Xxer1czZ87U4MGD1a1bN6Wlpen+++/X0aNH/eY413FfuHCh3xgb80nSAw880Gbt48aN8xtj8/GTLp7xXNfJqKgoPf30074xNh/D9tw3BOu2c/PmzbrhhhvkdDp1zTXXaMWKFcEJEZQP1wlzr732momNjTUvvfSS2b9/v5kyZYpJTEw0tbW1nb20i8rNzTXLly83+/btM3v27DG33367ycjIMCdPnvSN+eY3v2mmTJlijh075vtqaGjw7T9z5owZNGiQyc7ONu+//75Zu3at6dWrlykpKemMSG3MmTPHXHfddX7r/8tf/uLb/93vftekp6ebTZs2md27d5tRo0aZr3/96779tuerq6vzy1ZeXm4kmXfffdcYE57Hb+3ateaf//mfza9//Wsjybz++ut++xcuXGgSEhLM6tWrze9//3vz7W9/2/Tt29d89tlnvjHjxo0z119/vdm+fbv5zW9+Y6655hpzzz33+PY3NDSYlJQUU1BQYPbt22d+8YtfmK5du5oXX3yxU/PV19eb7Oxs88tf/tL84Q9/MBUVFWbkyJFm+PDhfnNkZmaaefPm+R3XL15vbc1njDGFhYVm3Lhxfms/fvy43xibj58xF8/4xWzHjh0zL730komKijKHDx/2jbH5GLbnviEYt51//OMfTXx8vCkuLjYHDhwwzz//vImJiTHr1q275AwUFGPMyJEjTVFRke/02bNnTVpamlmwYEEnrqpj6urqjCSzZcsW37ZvfvOb5uGHHz7vedauXWuio6NNTU2Nb9uSJUuMy+UyHo8nlMttlzlz5pjrr7/+nPvq6+uNw+Ewq1at8m378MMPjSRTUVFhjLE/35c9/PDD5uqrrzYtLS3GmPA/fl++8W9paTGpqanm6aef9m2rr683TqfT/OIXvzDGGHPgwAEjyezatcs35u233zZRUVHmk08+McYY88ILL5jLLrvML+PMmTNNv379QpzI37nu3L5s586dRpL56KOPfNsyMzPN4sWLz3sem/MVFhaaCRMmnPc84XT8jGnfMZwwYYIZPXq037ZwOYbGtL1vCNZt52OPPWauu+46v8u6++67TW5u7iWv+Sv/FE9zc7MqKyuVnZ3t2xYdHa3s7GxVVFR04so6pqGhQZKUlJTkt/3VV19Vr169NGjQIJWUlKipqcm3r6KiQoMHD/b7I3m5ublqbGzU/v37/zYLv4iDBw8qLS1NV111lQoKClRdXS1JqqyslNfr9Tt+/fv3V0ZGhu/4hUO+Vs3NzXrllVf0D//wD34fhBnux++Ljhw5opqaGr9jlpCQoKysLL9jlpiYqBEjRvjGZGdnKzo6Wjt27PCNueWWWxQbG+sbk5ubq6qqKv31r3/9G6Vpn4aGBkVFRbX5DLGFCxeqZ8+eGjZsmJ5++mm/h85tz7d582YlJyerX79+mjZtmj799FPfvkg7frW1tXrrrbc0efLkNvvC5Rh++b4hWLedFRUVfnO0jgnG/WdYfJpxKP3P//yPzp492+Yv2KakpOgPf/hDJ62qY1paWvTII4/opptu0qBBg3zb7733XmVmZiotLU0ffPCBZs6cqaqqKv3617+WJNXU1Jwzf+u+zpaVlaUVK1aoX79+OnbsmObOnatvfOMb2rdvn2pqahQbG9vmhj8lJcW3dtvzfdHq1atVX1+vBx54wLct3I/fl7Wu6Vxr/uIxS05O9tvfpUsXJSUl+Y3p27dvmzla91122WUhWX+gTp8+rZkzZ+qee+7x++C173//+7rhhhuUlJSkbdu2qaSkRMeOHdMzzzwjye5848aN08SJE9W3b18dPnxYP/7xjzV+/HhVVFQoJiYmoo6fJL388svq0aOHJk6c6Lc9XI7hue4bgnXbeb4xjY2N+uyzz9S1a9cOr/srX1AiSVFRkfbt26f33nvPb/vUqVN93w8ePFi9e/fWmDFjdPjwYV199dV/62UGbPz48b7vhwwZoqysLGVmZupXv/rVJf3y22jZsmUaP36830eRh/vx+yrzer36zne+I2OMlixZ4revuLjY9/2QIUMUGxurf/qnf9KCBQs6/U+MX8ykSZN83w8ePFhDhgzR1Vdfrc2bN2vMmDGduLLQeOmll1RQUKC4uDi/7eFyDM9332C7r/xTPL169VJMTEybVy7X1tYqNTW1k1YVuOnTp2vNmjV699131adPnwuOzcrKkiQdOnRIkpSamnrO/K37bJOYmKivfe1rOnTokFJTU9Xc3Kz6+nq/MV88fuGS76OPPtLGjRv1j//4jxccF+7Hr3VNF7rOpaamqq6uzm//mTNndPz48bA5rq3l5KOPPlJ5eflFP7Y+KytLZ86c0Z/+9CdJ9uf7oquuukq9evXy+50M9+PX6je/+Y2qqqouer2U7DyG57tvCNZt5/nGuFyuS/4P5Fe+oMTGxmr48OHatGmTb1tLS4s2bdokt9vdiStrH2OMpk+frtdff13vvPNOm4cTz2XPnj2SpN69e0uS3G639u7d63eD0nqDOnDgwJCs+1KcPHlShw8fVu/evTV8+HA5HA6/41dVVaXq6mrf8QuXfMuXL1dycrLy8vIuOC7cj1/fvn2Vmprqd8waGxu1Y8cOv2NWX1+vyspK35h33nlHLS0tvoLmdru1detWeb1e35jy8nL169ev058eaC0nBw8e1MaNG9WzZ8+LnmfPnj2Kjo72PTVic74v+/Of/6xPP/3U73cynI/fFy1btkzDhw/X9ddff9GxNh3Di903BOu20+12+83ROiYo95+X/DLbCPDaa68Zp9NpVqxYYQ4cOGCmTp1qEhMT/V65bKtp06aZhIQEs3nzZr+3ujU1NRljjDl06JCZN2+e2b17tzly5Ih54403zFVXXWVuueUW3xytbyXLyckxe/bsMevWrTOXX365NW/D/cEPfmA2b95sjhw5Yn7729+a7Oxs06tXL1NXV2eM+fytchkZGeadd94xu3fvNm6327jdbt/5bc9nzOfvHMvIyDAzZ8702x6ux+/EiRPm/fffN++//76RZJ555hnz/vvv+97FsnDhQpOYmGjeeOMN88EHH5gJEyac823Gw4YNMzt27DDvvfeeufbaa/3eplpfX29SUlLMfffdZ/bt22dee+01Ex8f/zd5C+eF8jU3N5tvf/vbpk+fPmbPnj1+18vWdz5s27bNLF682OzZs8ccPnzYvPLKK+byyy83999/v/X5Tpw4YX74wx+aiooKc+TIEbNx40Zzww03mGuvvdacPn3aN4fNx+9iGVs1NDSY+Ph4s2TJkjbnt/0YXuy+wZjg3Ha2vs14xowZ5sMPPzRlZWW8zTjYnn/+eZORkWFiY2PNyJEjzfbt2zt7Se0i6Zxfy5cvN8YYU11dbW655RaTlJRknE6nueaaa8yMGTP8/o6GMcb86U9/MuPHjzddu3Y1vXr1Mj/4wQ+M1+vthERt3X333aZ3794mNjbWXHHFFebuu+82hw4d8u3/7LPPzPe+9z1z2WWXmfj4eHPXXXeZY8eO+c1hcz5jjFm/fr2RZKqqqvy2h+vxe/fdd8/5e1lYWGiM+fytxo8//rhJSUkxTqfTjBkzpk32Tz/91Nxzzz2me/fuxuVymQcffNCcOHHCb8zvf/97c/PNNxun02muuOIKs3Dhwk7Pd+TIkfNeL1v/tk1lZaXJysoyCQkJJi4uzgwYMMA8+eSTfnfwtuZramoyOTk55vLLLzcOh8NkZmaaKVOmtPkPnc3H72IZW7344ouma9eupr6+vs35bT+GF7tvMCZ4t53vvvuuGTp0qImNjTVXXXWV32Vciqj/CwIAAGCNr/xrUAAAgH0oKAAAwDoUFAAAYB0KCgAAsA4FBQAAWIeCAgAArENBAQAA1qGgAAAA61BQAACAdSgoAADAOhQUAABgHQoKAACwzv8CDITbvFqi1RwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv('../../.env'))\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "client = OpenAI()\n",
    "openai.api_key ='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_txt = df.text[0]\n",
    "res = client.embeddings.create(input=tmp_txt, model='text-embedding-ada-002')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       research?authors=henk tillman.  Research index...\n",
       "1       Gupta (1)William Guss (1)Chris Hallacy (...\n",
       "2       Stanley (1)Jacob Steinhardt (1)Nisan Stien...\n",
       "3       research?authors=arthur petron.  Research inde...\n",
       "4       Gupta (1)William Guss (1)Chris Hallacy (...\n",
       "                              ...                        \n",
       "2020    Stanley (1)Jacob Steinhardt (1)Nisan Stien...\n",
       "2021    research?authors=chelsea voss.  Research index...\n",
       "2022    Gupta (1)William Guss (1)Chris Hallacy (...\n",
       "2023    Stanley (1)Jacob Steinhardt (1)Nisan Stien...\n",
       "2024    jobs.  Careers    CloseSearch Submit Skip to m...\n",
       "Name: text, Length: 2025, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df.text.apply(lambda x: client.embeddings.create(input=x, model='text-embedding-ada-002').data[0].embedding)\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=henk tillman.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.006426393985748291, -0.020367804914712906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.0030676156748086214, -0.003532822011038661,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.00471889041364193, -0.0065433862619102, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0                     research?authors=henk tillman.         9   \n",
       "1  Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2  Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...       195   \n",
       "3  Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "4                    research?authors=arthur petron.         9   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.006426393985748291, -0.020367804914712906,...  \n",
       "1  [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2  [0.0030676156748086214, -0.003532822011038661,...  \n",
       "3  [0.0025212429463863373, -0.0010925385868176818...  \n",
       "4  [-0.00471889041364193, -0.0065433862619102, -0...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>research?authors=henk tillman.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.006426393985748291, -0.020367804914712906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.0030676156748086214, -0.003532822011038661,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research?authors=arthur petron.</td>\n",
       "      <td>9</td>\n",
       "      <td>[-0.00471889041364193, -0.0065433862619102, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>AI is an extremely powerful tool that must be ...</td>\n",
       "      <td>263</td>\n",
       "      <td>[-0.003355710068717599, -0.03532751277089119, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>research?topics=meta learning.</td>\n",
       "      <td>7</td>\n",
       "      <td>[-0.01474149152636528, -0.004488331265747547, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>Bartlett (1)Mohammad Bavarian (2)Alexandre M B...</td>\n",
       "      <td>434</td>\n",
       "      <td>[0.001631682156585157, 0.0006636836915276945, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.00284423865377903, -0.003469044342637062, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...</td>\n",
       "      <td>215</td>\n",
       "      <td>[0.0025212429463863373, -0.0010925385868176818...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2025 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  n_tokens  \\\n",
       "0                        research?authors=henk tillman.         9   \n",
       "1     Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2     Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...       195   \n",
       "3     Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "4                       research?authors=arthur petron.         9   \n",
       "...                                                 ...       ...   \n",
       "2020  AI is an extremely powerful tool that must be ...       263   \n",
       "2021                     research?topics=meta learning.         7   \n",
       "2022  Bartlett (1)Mohammad Bavarian (2)Alexandre M B...       434   \n",
       "2023  Efros (1)Tyna Eloundou (3)lfar Erlingsson (1...       195   \n",
       "2024  Robinson (1)Nick Ryder (2)Ruslan Salakhutdinov...       215   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [-0.006426393985748291, -0.020367804914712906,...  \n",
       "1     [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2     [0.0030676156748086214, -0.003532822011038661,...  \n",
       "3     [0.0025212429463863373, -0.0010925385868176818...  \n",
       "4     [-0.00471889041364193, -0.0065433862619102, -0...  \n",
       "...                                                 ...  \n",
       "2020  [-0.003355710068717599, -0.03532751277089119, ...  \n",
       "2021  [-0.01474149152636528, -0.004488331265747547, ...  \n",
       "2022  [0.001631682156585157, 0.0006636836915276945, ...  \n",
       "2023  [0.00284423865377903, -0.003469044342637062, 0...  \n",
       "2024  [0.0025212429463863373, -0.0010925385868176818...  \n",
       "\n",
       "[2025 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800\n",
    "):\n",
    "    \"\"\"\n",
    "             .\n",
    "    \"\"\"\n",
    "\n",
    "    #   .\n",
    "    q_embeddings = client.embeddings.create(input=question, model='text-embedding-ada-002').data[0].embedding\n",
    "    \n",
    "    # q_embeddings: \"   ?\" =>   .\n",
    "    \n",
    "    # df['embeddings']:   . =>   .\n",
    "\n",
    "    #   .\n",
    "    # cosine similarity:    .\n",
    "    df[\"distances\"] = df[\"embeddings\"].apply(lambda x: cosine(q_embeddings, x))\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "    #  ,       .\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "\n",
    "        #      .\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        #   , break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "\n",
    "        # ,  .\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)\n",
    "\n",
    "\n",
    "#      .\n",
    "# 1.    . ,   .\n",
    "# 2.        .\n",
    "# 3.    ,       .\n",
    "# 4.  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research?authors=jong wook kim.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=diane yoon.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=joanne jang.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=heewoo jun.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=cathy yeh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=reiichiro nakano.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=teddy lee.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=mark chen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yonadav shavit.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yura burda.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=yan duan.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=rachel lim.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=lama ahmad.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=peter chen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=nicolas papernot.\n",
      "\n",
      "###\n",
      "\n",
      "Kaelbling, Hyeonwoo Noh, Lerrel Pinto, John Schulman, Ilya Sutskever & Tao Xu.Video: Peter Jordan (Director), Yvette Solis (Producer), Brooke Chan (Producer)Editor: Ashley PilipiszynDesign: Justin Jay Wang & Ben BarryPhotography: Eric HainesResearchOverviewIndexGPT-4DALLE 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI  20152024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top .\n",
      "\n",
      "###\n",
      "\n",
      "Kaelbling, Hyeonwoo Noh, Lerrel Pinto, John Schulman, Ilya Sutskever & Tao Xu.Video: Peter Jordan (Director), Yvette Solis (Producer), Brooke Chan (Producer)Editor: Ashley PilipiszynDesign: Justin Jay Wang & Ben BarryPhotography: Eric HainesResearchOverviewIndexGPT-4DALLE 3APIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI  20152024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top .\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=heidy khlaaf.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=susan zhang.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lama ahmad.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=rewon child.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=irene solaiman.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=brooke chan.\n",
      "\n",
      "###\n",
      "\n",
      "blog.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=kevin frans.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=ariel herbert voss.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=rachel fong.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=benjamin mann.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=lilian weng.\n",
      "\n",
      "###\n",
      "\n",
      "careers search?c=security.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=nisan stiennon.\n",
      "\n",
      "###\n",
      "\n",
      "enterprise.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=leo gao.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=gabriel goh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=catherine yeh.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=jade leung.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lilian weng.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=arvind neelakantan.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=vicki cheung.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=teddy lee.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jan hendrik kirchner.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=sandhini agarwal.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jonas schneider.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jonathan ho.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=lerrel pinto.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=jan leike.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=tyna eloundou.\n",
      "\n",
      "###\n",
      "\n",
      "The board of Almannarmur.English to IcelandicPromptHva heitir Donald Duck  slensku?What is Donald Duck called in Icelandic?GPT-3 completionDonald Duck heitir  slensku Donaldi KjniDonald Duck is called in Icelandic Donaldi the FoolIncorrectChatGPT completionDonald Duck heitir sama nafni  slensku og enskuDonald Duck has the same name in Icelandic and EnglishIncorrectGPT-4 completionDonald Duck heitir Andrs nd  slenskuDonald Duck is called Andrs nd in IcelandicCorrectEven GPT-4, while much better at Icelandic than GPT-3.5, produces Icelandic with grammatical errors, translationese, and incorrect cultural knowledge. To change this, Vilhjlmur orsteinsson, chief executive at Mieind ehf, a language technology company involved in the initiative, assembled a team of 40 volunteers to train GPT-4 on proper Icelandic grammar and cultural knowledge.In a process called Reinforcement Learning from Human Feedback, or RLHF, human testers give GPT-4 a prompt, and four possible completions are generated. Testers then select the best answer from the four responses and edit it to create an ideal completion. The data from this process is then used to further train GPT-4 to produce better responses in the future.Prior to RLHF, the process of fine-tuning a model was labor and data-intensive.\n",
      "\n",
      "###\n",
      "\n",
      "The board of Almannarmur.English to IcelandicPromptHva heitir Donald Duck  slensku?What is Donald Duck called in Icelandic?GPT-3 completionDonald Duck heitir  slensku Donaldi KjniDonald Duck is called in Icelandic Donaldi the FoolIncorrectChatGPT completionDonald Duck heitir sama nafni  slensku og enskuDonald Duck has the same name in Icelandic and EnglishIncorrectGPT-4 completionDonald Duck heitir Andrs nd  slenskuDonald Duck is called Andrs nd in IcelandicCorrectEven GPT-4, while much better at Icelandic than GPT-3.5, produces Icelandic with grammatical errors, translationese, and incorrect cultural knowledge. To change this, Vilhjlmur orsteinsson, chief executive at Mieind ehf, a language technology company involved in the initiative, assembled a team of 40 volunteers to train GPT-4 on proper Icelandic grammar and cultural knowledge.In a process called Reinforcement Learning from Human Feedback, or RLHF, human testers give GPT-4 a prompt, and four possible completions are generated. Testers then select the best answer from the four responses and edit it to create an ideal completion. The data from this process is then used to further train GPT-4 to produce better responses in the future.Prior to RLHF, the process of fine-tuning a model was labor and data-intensive.\n",
      "\n",
      "###\n",
      "\n",
      "scholars.    \n",
      "\n",
      "###\n",
      "\n",
      "careers search.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=john schulman 2.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=erika reinhardt.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=long ouyang.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=ilya sutskever.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=pranav shyam.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=alex paino.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=jan leike.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=harold nguyen.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=oleg klimov.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=sandhini agarwal.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=melanie subbiah.\n",
      "\n",
      "###\n",
      "\n",
      "blog?authors=vik goel.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=casey chu.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=ilya sutskever.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=aditya ramesh.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=clemens winter.\n",
      "\n",
      "###\n",
      "\n",
      "research?authors=mateusz litwin.\n"
     ]
    }
   ],
   "source": [
    "print(create_context('  ?', df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "            .\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n---\\n\\n Question: {question}\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What day is it?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The newest embeddings model is called \"text-embedding-ada-002\".'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is newest embeddings model?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, the pricing for using GPT-4 is as follows:\\n\\n- GPT-4 Turbo 8K Input: $0.03 Output: $0.06\\n- GPT-4 Turbo 32K Input: $0.06 Output: $0.12\\n- GPT-4 Turbo 128K Input: $0.01 Output: $0.03\\n\\nPlease note that these prices are per 1,000 tokens.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"How much does it cost to use gpt-4?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"   ?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \"text-embedding-ada-002\".'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is newest embeddings model?  \", debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05f34a34d73b71652304030c1097be3a5720ea2447153dd6542d145a26b73181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
